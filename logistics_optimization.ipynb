{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 1: Data Preparation\n",
    "# Project: Predictive Analytics for Supply Chain Optimization\n",
    "# ====================================================\n",
    "\n",
    "# --- 1/1. Import Required Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6910ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/2. Import data ---\n",
    "try:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='utf-8')\n",
    "  cities_df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\uscities.csv\")\n",
    "except UnicodeDecodeError:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='latin-1')\n",
    "  cities_df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\uscities.csv\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "\n",
    "# Preview\n",
    "# df.head()\n",
    "# df_city.head()\n",
    "\n",
    "\n",
    "# Show list of columns\n",
    "# print(df_city.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/3. Basic Data Overview ---\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "# print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nMissing values summary:\\n\", df.isna().sum())\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/4. Timestamp for testing ---\n",
    "# Choose range and count\n",
    "start_date = pd.to_datetime(\"2023-01-01\")\n",
    "end_date = pd.to_datetime(\"2024-12-31\")\n",
    "\n",
    "# Replace timestamp column with random dates\n",
    "df['timestamp'] = pd.to_datetime(\n",
    "    np.random.randint(\n",
    "        start_date.value // 10**9,\n",
    "        end_date.value // 10**9,\n",
    "        len(df)\n",
    "    ),\n",
    "    unit='s'\n",
    ")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/5. Handle Missing Values with NumPy ---\n",
    "# Identify numeric columns, but exclude 'timestamp'\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != 'timestamp']\n",
    "\n",
    "# Identify categorical columns\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Impute numeric columns with median\n",
    "for col in num_cols:\n",
    "    median_val = np.nanmedian(df[col])\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# If 'timestamp' is datetime and has missing values, optionally impute separately\n",
    "if df['timestamp'].isna().any():\n",
    "    # Replace NaT with some default, e.g., first date\n",
    "    df['timestamp'] = df['timestamp'].fillna(df['timestamp'].min())\n",
    "\n",
    "\n",
    "# print(df['timestamp'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/7 Nearest City Assignment Using KDTree ---\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Example: your cities DataFrame\n",
    "# cities = pd.read_csv(\"uscities.csv\")\n",
    "# Keep only necessary columns\n",
    "cities = cities_df[['city', 'state_id', 'lat', 'lng']]\n",
    "\n",
    "# Build KDTree\n",
    "city_coords = cities_df[['lat', 'lng']].values\n",
    "tree = cKDTree(city_coords)\n",
    "\n",
    "# Vehicle coordinates\n",
    "vehicle_coords = df[['vehicle_gps_latitude', 'vehicle_gps_longitude']].values\n",
    "\n",
    "# Query nearest city (k=1 for nearest)\n",
    "distances, idx = tree.query(vehicle_coords, k=1)\n",
    "\n",
    "# Optional: compute approximate distance in km (using Haversine)\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of earth in km\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Compute distances for all rows\n",
    "vehicle_lat = df['vehicle_gps_latitude'].values\n",
    "vehicle_lon = df['vehicle_gps_longitude'].values\n",
    "city_lat = cities_df.iloc[idx]['lat'].values\n",
    "city_lon = cities_df.iloc[idx]['lng'].values\n",
    "\n",
    "dist_km = haversine(vehicle_lat, vehicle_lon, city_lat, city_lon)\n",
    "\n",
    "# Set threshold, e.g., 50 km\n",
    "threshold_km = 50\n",
    "\n",
    "# Assign vehicle_city only if within threshold\n",
    "# Pre-build city + state\n",
    "cities_df[\"city_full\"] = cities_df[\"city\"].astype(str) + \", \" + cities_df[\"state_id\"].astype(str)\n",
    "\n",
    "# Assign nearest city only when it is within threshold\n",
    "df[\"vehicle_city\"] = np.where(\n",
    "    dist_km <= threshold_km,\n",
    "    cities_df.iloc[idx][\"city_full\"].values,\n",
    "    np.nan\n",
    ")\n",
    "# Remove rows with any NaN values\n",
    "# df_clean = df.dropna()\n",
    "df = df.dropna()\n",
    "\n",
    "# Check the result\n",
    "# print(df_clean.shape)  # number of rows and columns after dropping\n",
    "# print(df['vehicle_city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/8 Add a unique ID for each row\n",
    "df['row_id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Check\n",
    "# print(df[['row_id', 'vehicle_gps_latitude', 'vehicle_gps_longitude']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/9 Feature Engineering ---\n",
    "\n",
    "## 5.1 Create derived features\n",
    "# Example: combine GPS into one column\n",
    "df['vehicle_location'] = df['vehicle_gps_latitude'].astype(str) + ',' + df['vehicle_gps_longitude'].astype(str)\n",
    "\n",
    "# Example: compute delay difference in minutes\n",
    "df['eta_variation_minutes'] = df['eta_variation_hours'] * 60\n",
    "\n",
    "# Example: categorize traffic level\n",
    "df['traffic_category'] = pd.cut(\n",
    "    df['traffic_congestion_level'],\n",
    "    bins=[0, 3, 6, 10],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Example: flag if weather severity exceeds threshold\n",
    "df['severe_weather_flag'] = np.where(df['weather_condition_severity'] > 0.7, 1, 0)\n",
    "\n",
    "# ===========================================================\n",
    "print(\"\\n Feature engineering complete. New columns added:\", \n",
    "      [c for c in df.columns if 'vehicle_location' in c or 'eta_variation_minutes' in c or 'traffic_category' in c or 'severe_weather_flag' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/10. Clustering ---\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# --- Temporal features ---\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['eta_minus_actual_hours'] = df['eta_variation_hours'] - df['delivery_time_deviation']\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df = df.sort_values(['row_id', 'timestamp'])\n",
    "\n",
    "df['avg_route_congestion'] = (\n",
    "    df.groupby('route_risk_level')['traffic_congestion_level']\n",
    "      .transform(lambda x: x.rolling(5).mean())\n",
    ")\n",
    "\n",
    "# --- Clustering ---\n",
    "coords = df[['vehicle_gps_latitude', 'vehicle_gps_longitude']]\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['route_cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# --- Cluster statistics ---\n",
    "cluster_stats = (\n",
    "    df.groupby('route_cluster')\n",
    "      .agg(\n",
    "          avg_delay=('delivery_time_deviation', 'mean'),\n",
    "          avg_risk=('route_risk_level', 'mean'),\n",
    "          avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "          lat=('vehicle_gps_latitude', 'mean'),\n",
    "          lon=('vehicle_gps_longitude', 'mean')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Determine geographic labels\n",
    "lat_median = cluster_stats['lat'].median()\n",
    "lon_median = cluster_stats['lon'].median()\n",
    "\n",
    "def geo_label(row):\n",
    "    parts = []\n",
    "    # North / South\n",
    "    parts.append(\"Northern\" if row['lat'] > lat_median else \"Southern\")\n",
    "    # East / West\n",
    "    parts.append(\"Eastern\" if row['lon'] > lon_median else \"Western\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "cluster_stats['geo'] = cluster_stats.apply(geo_label, axis=1)\n",
    "\n",
    "# Categorize delay\n",
    "def delay_label(v):\n",
    "    return \"High-delay\" if v > cluster_stats['avg_delay'].median() else \"Stable-delivery\"\n",
    "\n",
    "# Categorize risk\n",
    "def risk_label(v):\n",
    "    return \"High-risk\" if v > cluster_stats['avg_risk'].median() else \"Low-risk\"\n",
    "\n",
    "# Congestion\n",
    "def congestion_label(v):\n",
    "    return \"Congested\" if v > cluster_stats['avg_congestion'].median() else \"Free-flow\"\n",
    "\n",
    "cluster_stats['delay_text'] = cluster_stats['avg_delay'].apply(delay_label)\n",
    "cluster_stats['risk_text'] = cluster_stats['avg_risk'].apply(risk_label)\n",
    "cluster_stats['cong_text'] = cluster_stats['avg_congestion'].apply(congestion_label)\n",
    "\n",
    "# FINAL NAME\n",
    "cluster_stats['cluster_name'] = (\n",
    "    cluster_stats['geo'] + \" ‚Äî \" +\n",
    "    cluster_stats['delay_text'] + \", \" +\n",
    "    cluster_stats['risk_text'] + \", \" +\n",
    "    cluster_stats['cong_text']\n",
    ")\n",
    "\n",
    "# Map into df\n",
    "name_map = cluster_stats.set_index('route_cluster')['cluster_name'].to_dict()\n",
    "df['cluster_desc'] = df['route_cluster'].map(name_map)\n",
    "\n",
    "cluster_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/11. Categorical columns ---\n",
    "# Identify which columns are categorical\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "print(\"Categorical columns:\", list(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/12. Categorical columns ---\n",
    "# Categorical columns ‚Üí replace NaN with most frequent value (mode)\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "print(\"\\n Missing values handled with NumPy (median/mode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/14. Final Dataset Summary ---\n",
    "print(\"\\nFinal dataset shape:\", df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "display(df.head())\n",
    "# print(df.shipping_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "# ====================================================\n",
    "\n",
    "# --- 2/1. Import Required Libraries ---\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Assume df is already loaded and cleaned from Phase 1\n",
    "print(\" DataFrame available with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40512d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/2. Route clusters visualization---\n",
    "# Ensure necessary columns exist\n",
    "required_cols = [\n",
    "    'vehicle_gps_latitude',\n",
    "    'vehicle_gps_longitude',\n",
    "    'route_cluster',\n",
    "    'delivery_time_deviation'\n",
    "]\n",
    "\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='vehicle_gps_longitude',\n",
    "    y='vehicle_gps_latitude',\n",
    "    hue='cluster_desc',      # ‚úî –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤ –ª–µ–≥–µ–Ω–¥–µ\n",
    "    size='delivery_time_deviation',\n",
    "    palette='tab10',\n",
    "    sizes=(40, 300),\n",
    "    alpha=0.8,\n",
    "    legend=\"brief\"\n",
    ")\n",
    "\n",
    "plt.title(\"Route Clusters Based on GPS Coordinates\\n(Bubble Size = Delivery Time Deviation)\", fontsize=16)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/3. Prepare stat data ---\n",
    "# Define the feature columns (independent variables)\n",
    "features = [\n",
    "    \"fuel_consumption_rate\", \"traffic_congestion_level\", \"weather_condition_severity\",\n",
    "    \"warehouse_inventory_level\", \"loading_unloading_time\", \"handling_equipment_availability\",\n",
    "    'route_cluster', \"port_congestion_level\", \"shipping_costs\", \"supplier_reliability_score\", \"\"\n",
    "    \"lead_time_days\", \"route_risk_level\", \"driver_behavior_score\", \"fatigue_monitoring_score\"\n",
    "]\n",
    "\n",
    "# Define the target column (dependent variable)\n",
    "# ===============================================\n",
    "target = \"delivery_time_deviation\"\n",
    "# ===============================================\n",
    "\n",
    "# Keep only the selected columns and remove rows with missing data\n",
    "stat_df = df[features + [target]].dropna()\n",
    "\n",
    "# Check the dataset after cleaning\n",
    "print(f\"Data after filtering: {stat_df.shape[0]:,} rows and {stat_df.shape[1]} columns\")\n",
    "\n",
    "# Display summary statistics of the cleaned dataset\n",
    "stat_df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- 2.3 Scale Numerical Features ---\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()  # default range is 0 to 1\n",
    "stat_df[features] = scaler.fit_transform(stat_df[features])\n",
    "print(stat_df[features].head())\n",
    "\n",
    "\n",
    "# Check result\n",
    "print(stat_df[features].head())\n",
    "print(\"\\nCategorical encoding and scaling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2692f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/5. Data Correlation ---\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assume stat_df has the same feature and target columns\n",
    "# -----------------------------\n",
    "# 1Ô∏è Spearman correlation (monotonic relationships)\n",
    "spearman_corr = stat_df.corr(method='spearman')[target].drop(target).sort_values(ascending=False)\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=spearman_corr.values, y=spearman_corr.index, hue=spearman_corr.index, dodge=False, palette=\"viridis\", legend=False)\n",
    "plt.title(\"Spearman Correlation with Delivery Time Deviation (stat_df)\")\n",
    "plt.xlabel(\"Spearman Correlation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è Mutual Information (non-linear dependencies)\n",
    "X_stat = stat_df[features]\n",
    "y_stat = stat_df[target]\n",
    "mi = mutual_info_regression(X_stat, y_stat, random_state=42)\n",
    "mi_series = pd.Series(mi, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=mi_series.values,\n",
    "    y=mi_series.index,\n",
    "    hue=mi_series.index,   # required for palette\n",
    "    dodge=False,\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Mutual Information with Delivery Time Deviation (stat_df)\")\n",
    "plt.xlabel(\"Mutual Information\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è Feature importance using Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_stat, y_stat)\n",
    "rf_importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=rf_importances.values, y=rf_importances.index, hue=spearman_corr.index, dodge=False, palette=\"magma\", legend=False)\n",
    "# sns.barplot(x=rf_importances.values, y=rf_importances.index, palette=\"magma\")\n",
    "plt.title(\"Random Forest Feature Importance (stat_df)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3Ô∏è DISTRIBUTION ANALYSIS AND ANOMALY DETECTION\n",
    "# ====================================================\n",
    "# --- 3/1. Plot distribution of key numeric variables---\n",
    "\n",
    "key_vars = [\n",
    "    'delivery_time_deviation',\n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "\n",
    "key_vars = [v for v in key_vars if v in df.columns]\n",
    "print(key_vars)\n",
    "\n",
    "for col in key_vars:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/2. Detect anomalies using IQR method for each key numeric column---\n",
    "\n",
    "key_vars = [    \n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "\n",
    "\n",
    "# --- Boxplot for key numeric columns ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=stat_df[key_vars], orient='h', palette='Set2')\n",
    "plt.title(\"Boxplot of Key Numeric Variables (Outliers shown)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c234cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/3. Remove anomalies using different methods and replace the data in primary df---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "# --- Step 0: Define variables ---\n",
    "skewed_vars = ['fuel_consumption_rate', 'lead_time_days']\n",
    "robust_var = 'fuel_consumption_rate'\n",
    "standard_vars = [v for v in stat_df.columns if v not in skewed_vars]\n",
    "\n",
    "# --- Step 1: Log-transform skewed features ---\n",
    "for col in skewed_vars:\n",
    "    if col in stat_df.columns:\n",
    "        stat_df[col] = np.log1p(stat_df[col])\n",
    "\n",
    "# --- Step 2: Outlier treatment for fuel_consumption_rate ---\n",
    "Q1 = stat_df[robust_var].quantile(0.25)\n",
    "Q3 = stat_df[robust_var].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_whisker = Q1 - 1.5 * IQR\n",
    "upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "# Option 1: Remove outliers\n",
    "# stat_df = stat_df[(stat_df[robust_var] >= lower_whisker) & (stat_df[robust_var] <= upper_whisker)]\n",
    "\n",
    "# Option 2: Cap outliers (recommended to keep dataset size)\n",
    "stat_df[robust_var] = np.where(\n",
    "    stat_df[robust_var] > upper_whisker, upper_whisker,\n",
    "    np.where(stat_df[robust_var] < lower_whisker, lower_whisker, stat_df[robust_var])\n",
    ")\n",
    "\n",
    "# --- Step 3: Apply RobustScaler to fuel_consumption_rate ---\n",
    "robust_scaler = RobustScaler()\n",
    "stat_df[robust_var] = robust_scaler.fit_transform(stat_df[[robust_var]])\n",
    "\n",
    "# --- Step 4: Apply StandardScaler to other features ---\n",
    "if standard_vars:\n",
    "    standard_scaler = StandardScaler()\n",
    "    stat_df[standard_vars] = standard_scaler.fit_transform(stat_df[standard_vars])\n",
    "\n",
    "# preserve df for machine learning\n",
    "m_df = df.copy()\n",
    "\n",
    "# --- Step 5: Replace processed columns back into primary DataFrame safely ---\n",
    "for col in stat_df.columns:\n",
    "    if col in df.columns:\n",
    "        df.loc[stat_df.index, col] = stat_df[col]\n",
    "\n",
    "# --- Optional: Check the first few rows ---\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/4 Boxplot for key numeric columns ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=stat_df[key_vars], orient='h', palette='Set2')\n",
    "plt.title(\"Boxplot of Key Numeric Variables (Outliers shown)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/5 Summary statistics by cluster_desc\n",
    "summary_by_cluster = df.groupby('cluster_desc').agg(\n",
    "    avg_delay=('delivery_time_deviation', 'mean'),\n",
    "    avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "    avg_fuel=('fuel_consumption_rate', 'mean'),\n",
    "    avg_shipping=('shipping_costs', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot: Average delivery delay by cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='avg_delay',\n",
    "    y='cluster_desc',\n",
    "    hue='cluster_desc',        # set hue same as y\n",
    "    data=summary_by_cluster.sort_values('avg_delay', ascending=False),\n",
    "    palette='Reds_r',\n",
    "    dodge=False,               # avoid splitting bars\n",
    "    legend=False               # hide the extra legend\n",
    ")\n",
    "plt.title(\"Average Delivery Delay by Cluster\")\n",
    "plt.xlabel(\"Average Delivery Time Deviation (hours)\")\n",
    "plt.ylabel(\"Cluster Description\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/13. Save Cleaned Dataset ---\n",
    "df.to_csv('cleaned_logistics_data.csv', index=False)\n",
    "print(\"\\n Cleaned dataset saved as 'cleaned_logistics_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 4Ô∏è SEASONAL & REGIONAL DELAY TRENDS\n",
    "# ====================================================\n",
    "# --- 4/1. fuel consumption rate by month---\n",
    "\n",
    "#correlation between fuel consuntion rate and temperature inside\n",
    "\n",
    "\n",
    "# Convert timestamp to datetime if needed\n",
    "if not np.issubdtype(df['timestamp'].dtype, np.datetime64):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Extract time-based features\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# --- 4.1 Seasonal trends (monthly average delay)\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    monthly_delay = df.groupby('month')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=monthly_delay.index, y=monthly_delay.values, marker='o')\n",
    "    plt.title(\" Average Delivery Time Deviation by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Weekday delay trends\n",
    "\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "# Define correct order\n",
    "    weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "    # Convert to categorical with order\n",
    "    df['weekday'] = pd.Categorical(df['weekday'], categories=weekday_order, ordered=True)\n",
    "\n",
    "    # Group and sort\n",
    "    weekday_delay = df.groupby('weekday', observed=False)['delivery_time_deviation'].mean().sort_index()\n",
    "\n",
    "    # print(weekday_delay)\n",
    "\n",
    "    # weekday_delay = df.groupby('weekday')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    sns.barplot(\n",
    "    x=weekday_delay.index,\n",
    "    y=weekday_delay.values,\n",
    "    hue=weekday_delay.values,  # use values to color bars\n",
    "    palette='crest',\n",
    "    legend=False               # hide legend\n",
    "    )\n",
    "\n",
    "    plt.title(\" Average Delivery Deviation by Weekday\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Phase 2: EDA completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 5: Machine Learning Modeling\n",
    "# ====================================================\n",
    "# 5/0 FEATURE SELECTION & ENCODING\n",
    "\n",
    "# Drop non-numeric / irrelevant columns\n",
    "exclude_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname', 'vehicke_city', 'traffic_category']\n",
    "m_df = m_df.drop(columns=[c for c in exclude_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in m_df.select_dtypes(include='object').columns:\n",
    "    m_df[col] = LabelEncoder().fit_transform(m_df[col].astype(str))\n",
    "\n",
    "# Replace remaining NaNs with median\n",
    "m_df = m_df.fillna(df.median(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 DATA CLEANUP BEFORE MODELING\n",
    "\n",
    "# Drop identifier or irrelevant columns if they exist\n",
    "drop_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email', \n",
    "    'Customer Fname', 'Customer Lname', 'vehicle_city', 'weekday'\n",
    "]\n",
    "m_df = m_df.drop(columns=[c for c in drop_cols if c in m_df.columns], errors='ignore')\n",
    "\n",
    "# --- 1. Convert booleans to int\n",
    "bool_cols = m_df.select_dtypes(include=['bool']).columns\n",
    "if len(bool_cols):\n",
    "    data[bool_cols] = data[bool_cols].astype(int)\n",
    "\n",
    "# --- 2. Encode categoricals (object / category)\n",
    "cat_cols = m_df.select_dtypes(include=['object', 'category']).columns\n",
    "if len(cat_cols):\n",
    "    print(\"Encoding categorical columns:\", list(cat_cols))\n",
    "    for col in cat_cols:\n",
    "        data[col] = data[col].astype(str).astype('category').cat.codes\n",
    "\n",
    "# --- 3. Replace any infinite or missing values\n",
    "m_df = m_df.replace([np.inf, -np.inf], np.nan)\n",
    "m_df = m_df.fillna(m_df.median(numeric_only=True))\n",
    "\n",
    "# --- 4. Enforce numeric dtype globally\n",
    "# m_df = m_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "# --- 5. Verify everything is numeric\n",
    "non_numeric = m_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\" Still non-numeric columns:\", non_numeric)\n",
    "else:\n",
    "    print(\" All columns successfully converted to numeric.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a633d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 checking df\n",
    "\n",
    "# checking dataframe\n",
    "print(\"m_df shape:\", m_df.shape)\n",
    "print(\"X shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5/3 Regression Modeling ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Target column ---\n",
    "target_reg = \"delivery_time_deviation\"\n",
    "\n",
    "# --- Copy numeric features ---\n",
    "numeric_cols = m_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove the target column\n",
    "if 'delivery_time_deviation' in numeric_cols:\n",
    "    numeric_cols.remove('delivery_time_deviation')\n",
    "X = m_df[numeric_cols].copy()\n",
    "y_reg = m_df[target_reg]\n",
    "\n",
    "# --- 1. Log-transform skewed features ---\n",
    "for col in ['fuel_consumption_rate', 'lead_time_days']:\n",
    "    if col in X.columns:\n",
    "        X[col] = np.log1p(X[col])  # safe for 0 values\n",
    "\n",
    "# --- 2. Cap outliers in fuel_consumption_rate ---\n",
    "if 'fuel_consumption_rate' in X.columns:\n",
    "    Q1 = X['fuel_consumption_rate'].quantile(0.25)\n",
    "    Q3 = X['fuel_consumption_rate'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "    X['fuel_consumption_rate'] = np.where(\n",
    "        X['fuel_consumption_rate'] > upper_whisker, upper_whisker,\n",
    "        np.where(X['fuel_consumption_rate'] < lower_whisker, lower_whisker, X['fuel_consumption_rate'])\n",
    "    )\n",
    "\n",
    "# --- 3. Impute missing values ---\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# --- 4. Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# --- 5. Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# --- 6. Define regression models ---\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Train models and evaluate ---\n",
    "results_reg = {}\n",
    "print(\"Training regression models...\")\n",
    "for name in tqdm(models_reg.keys(), desc=\"Training Progress\", leave=False):\n",
    "    model = models_reg[name]\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "    print(f\"\\n{name} done.\")\n",
    "    print(f\"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "# --- Display results ---\n",
    "results_reg_df = pd.DataFrame(results_reg).T\n",
    "display(results_reg_df.style.background_gradient(cmap=\"Blues\").format(\"{:.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d85055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost lightgbm matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global storage for trained models---\n",
    "trained_models = {\n",
    "    \"regression\": {},\n",
    "    \"classification\": {}\n",
    "}\n",
    "print(\"\\n Trained models stored in 'trained_models' dictionary.\")\n",
    "print(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5/3: Machine Learning Random Forest Regressor ---\n",
    "\n",
    "# Required imports\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0Ô∏è  Create trained models container\n",
    "# -------------------------------------------------------------------\n",
    "trained_models = {\n",
    "    \"regression\": {},\n",
    "    \"classification\": {}\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1Ô∏è  PREPROCESSING \n",
    "# -------------------------------------------------------------------\n",
    "df_ml = m_df.copy()\n",
    "\n",
    "# ---- Drop problematic / ID columns ----\n",
    "drop_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email',\n",
    "    'Customer Fname', 'Customer Lname', 'disruption_likelihood_score'\n",
    "]\n",
    "\n",
    "df_ml = df_ml.drop(columns=[c for c in drop_cols if c in df_ml.columns], errors='ignore')\n",
    "\n",
    "# ---- Convert booleans to ints ----\n",
    "bool_cols = df_ml.select_dtypes(include='bool').columns\n",
    "df_ml[bool_cols] = df_ml[bool_cols].astype(int)\n",
    "\n",
    "# ---- Encode categorical ----\n",
    "cat_cols = df_ml.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df_ml[col] = LabelEncoder().fit_transform(df_ml[col].astype(str))\n",
    "\n",
    "# ---- Fill NaN using median (numerical only) ----\n",
    "for col in df_ml.columns:\n",
    "    if df_ml[col].dtype in [int, float]:\n",
    "        df_ml[col] = df_ml[col].fillna(df_ml[col].median())\n",
    "\n",
    "print(\"Preprocessing complete. Data ready for modeling.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2Ô∏è REGRESSION: delivery_time_deviation\n",
    "# -------------------------------------------------------------------\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\n Regression: delivery_time_deviation\")\n",
    "\n",
    "    X_reg = df_ml.drop(columns=['delivery_time_deviation'])\n",
    "    y_reg = df_ml['delivery_time_deviation']\n",
    "\n",
    "    # ---- Optional sampling for speed ----\n",
    "    sample_size = 5000\n",
    "    if len(X_reg) > sample_size:\n",
    "        X_reg = X_reg.sample(sample_size, random_state=42)\n",
    "        y_reg = y_reg.loc[X_reg.index]\n",
    "\n",
    "    # ---- Train/test split ----\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reg, y_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # ---- Scaling ----\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # ---- Regression Models ----\n",
    "    models_reg = {\n",
    "        \"Random Forest Regressor\": RandomForestRegressor(\n",
    "            n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "        )\n",
    "    }\n",
    "\n",
    "    results_reg = {}\n",
    "\n",
    "    for name in tqdm(models_reg.keys(), desc=\"Regression Models\", leave=False):\n",
    "        model = models_reg[name]\n",
    "        start = pd.Timestamp.now()\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "\n",
    "        print(f\"\\n {name} done in {elapsed:.2f}s | MAE:{mae:.3f} | RMSE:{rmse:.3f} | R¬≤:{r2:.3f}\")\n",
    "\n",
    "        trained_models[\"regression\"][name] = model\n",
    "\n",
    "        # ---- Feature importance ----\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            feat_imp = pd.Series(model.feature_importances_, index=X_reg.columns)\n",
    "            feat_imp.nlargest(10).plot(kind='barh', figsize=(8, 4),\n",
    "                title=f\"Top 10 Feature Importances ({name})\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3Ô∏è CLASSIFICATION\n",
    "# -------------------------------------------------------------------\n",
    "def run_classification(target_col):\n",
    "\n",
    "    print(f\"\\n Classification: {target_col}\")\n",
    "\n",
    "    if target_col not in df_ml.columns:\n",
    "        print(f\" Column '{target_col}' not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    X = df_ml.drop(columns=[target_col])\n",
    "    y = df_ml[target_col]\n",
    "\n",
    "    # Sample for speed\n",
    "    sample_size = 5000\n",
    "    if len(X) > sample_size:\n",
    "        X = X.sample(sample_size, random_state=42)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    # Encode y\n",
    "    y = LabelEncoder().fit_transform(y.astype(str))\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_cls = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(\n",
    "            n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "        )\n",
    "    }\n",
    "\n",
    "    for name in tqdm(models_cls.keys(), desc=f\"{target_col} Models\", leave=False):\n",
    "        model = models_cls[name]\n",
    "        start = pd.Timestamp.now()\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "\n",
    "        print(f\"\\n {name} done in {elapsed:.2f}s | Accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5/3 XGBoost and LightGBM for shipment disruption prediction\n",
    "\n",
    "\n",
    "sample_size = 1000\n",
    "\n",
    "if len(m_df) > sample_size:\n",
    "    df_sample = m_df.sample(sample_size, random_state=42)\n",
    "else:\n",
    "    df_sample = m_df.copy()\n",
    "\n",
    "X_sample = df_sample[features].values\n",
    "y_sample = df_sample[target].values\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Prepare features and target\n",
    "# -----------------------------\n",
    "features = [\n",
    "    'vehicle_gps_latitude', 'vehicle_gps_longitude', 'fuel_consumption_rate',\n",
    "    'eta_variation_hours', 'traffic_congestion_level', 'warehouse_inventory_level',\n",
    "    'loading_unloading_time', 'handling_equipment_availability',\n",
    "    'weather_condition_severity', 'port_congestion_level', 'shipping_costs',\n",
    "    'supplier_reliability_score', 'lead_time_days', 'historical_demand',\n",
    "    'iot_temperature', 'cargo_condition_status', 'route_risk_level',\n",
    "    'customs_clearance_time', 'driver_behavior_score', 'fatigue_monitoring_score'\n",
    "]\n",
    "target = 'disruption_likelihood_score'\n",
    "\n",
    "features = [f for f in features if f in m_df.columns]  # keep only available features\n",
    "\n",
    "X = m_df[features].values\n",
    "y = m_df[target].values\n",
    "\n",
    "# Normalize numeric features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Train XGBoost Regressor\n",
    "# -----------------------------\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Train LightGBM Regressor\n",
    "# -----------------------------\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Evaluate models\n",
    "# -----------------------------\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    # Predicted vs Actual plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"{model_name}: Predicted vs Actual\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Residual plot\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(f\"{model_name}: Residuals Distribution\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate XGBoost\n",
    "evaluate_model(y_test, y_pred_xgb, \"XGBoost Regressor\")\n",
    "\n",
    "# Evaluate LightGBM\n",
    "evaluate_model(y_test, y_pred_lgb, \"LightGBM Regressor\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Feature Importance (Optional)\n",
    "# -----------------------------\n",
    "def plot_feature_importance(model, model_name, feature_names):\n",
    "    importance = model.feature_importances_\n",
    "    feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importance})\n",
    "    feat_imp = feat_imp.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x='importance', y='feature', data=feat_imp)\n",
    "    plt.title(f\"{model_name} Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_feature_importance(xgb_model, \"XGBoost\", features)\n",
    "    plot_feature_importance(lgb_model, \"LightGBM\", features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2942901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 6: Insights & Recommendations\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "print(\" Phase 4: Interpreting model outputs and generating recommendations\")\n",
    "\n",
    "# --- 1Ô∏è Regression Insights: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\nüîπ Regression Insights: Top drivers of delivery time deviation\")\n",
    "\n",
    "    # Use the last trained Random Forest model from Phase 3\n",
    "    rf_model = models_reg.get(\"Random Forest Regressor\", None)\n",
    "\n",
    "    if rf_model is not None:\n",
    "        # Feature importance\n",
    "        feat_imp = pd.Series(rf_model.feature_importances_, index=X_reg.columns).sort_values(ascending=False)\n",
    "        top_features = feat_imp.head(10)\n",
    "        print(\"\\nTop 10 features driving delivery time deviation:\")\n",
    "        display(top_features)\n",
    "\n",
    "        # Visualize\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(\n",
    "            x=top_features.values,\n",
    "            y=top_features.index,\n",
    "            hue=top_features.index,          # required to keep palette usage\n",
    "            dodge=False,\n",
    "            palette=\"viridis\",\n",
    "            legend=False                     # disable legend created by hue\n",
    "        )        \n",
    "        # plt.figure(figsize=(8,5))\n",
    "        # sns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\n",
    "        # plt.title(\"Top 10 Feature Importances ‚Äî Delivery Time Deviation\")\n",
    "        # plt.xlabel(\"Importance\")\n",
    "        # plt.ylabel(\"Feature\")\n",
    "        # plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features.index:\n",
    "            print(f\"- Monitor and optimize {feat} to reduce delivery time deviations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autowini_venv)",
   "language": "python",
   "name": "autowini_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
