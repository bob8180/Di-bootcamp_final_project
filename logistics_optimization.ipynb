{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 1: Data Preparation\n",
    "# Project: Predictive Analytics for Supply Chain Optimization\n",
    "# ====================================================\n",
    "\n",
    "# --- 1/1. Import Required Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6910ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/2. Import data ---\n",
    "try:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='utf-8')\n",
    "  cities_df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\uscities.csv\")\n",
    "except UnicodeDecodeError:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='latin-1')\n",
    "  cities_df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\uscities.csv\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "\n",
    "# Preview\n",
    "# df.head()\n",
    "# df_city.head()\n",
    "\n",
    "\n",
    "# Show list of columns\n",
    "# print(df_city.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/3. Basic Data Overview ---\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "# print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nMissing values summary:\\n\", df.isna().sum())\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/5. Handle Missing Values with NumPy for data integrity---\n",
    "# Identify numeric columns, but exclude 'timestamp'\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != 'timestamp']\n",
    "\n",
    "# Identify categorical columns\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Impute numeric columns with median\n",
    "for col in num_cols:\n",
    "    median_val = np.nanmedian(df[col])\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# If 'timestamp' is datetime and has missing values, optionally impute separately\n",
    "if df['timestamp'].isna().any():\n",
    "    # Replace NaT with some default, e.g., first date\n",
    "    df['timestamp'] = df['timestamp'].fillna(df['timestamp'].min())\n",
    "\n",
    "\n",
    "# print(df['timestamp'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/7 Nearest City Assignment Using KDTree ---\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Example: your cities DataFrame\n",
    "# cities = pd.read_csv(\"uscities.csv\")\n",
    "# Keep only necessary columns\n",
    "cities = cities_df[['city', 'state_id', 'lat', 'lng']]\n",
    "\n",
    "# Build KDTree\n",
    "city_coords = cities_df[['lat', 'lng']].values\n",
    "tree = cKDTree(city_coords)\n",
    "\n",
    "# Vehicle coordinates\n",
    "vehicle_coords = df[['vehicle_gps_latitude', 'vehicle_gps_longitude']].values\n",
    "\n",
    "# Query nearest city (k=1 for nearest)\n",
    "distances, idx = tree.query(vehicle_coords, k=1)\n",
    "\n",
    "# Optional: compute approximate distance in km (using Haversine)\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of earth in km\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Compute distances for all rows\n",
    "vehicle_lat = df['vehicle_gps_latitude'].values\n",
    "vehicle_lon = df['vehicle_gps_longitude'].values\n",
    "city_lat = cities_df.iloc[idx]['lat'].values\n",
    "city_lon = cities_df.iloc[idx]['lng'].values\n",
    "\n",
    "dist_km = haversine(vehicle_lat, vehicle_lon, city_lat, city_lon)\n",
    "\n",
    "# Set threshold, e.g., 50 km\n",
    "threshold_km = 50\n",
    "\n",
    "# Assign vehicle_city only if within threshold\n",
    "# Pre-build city + state\n",
    "cities_df[\"city_full\"] = cities_df[\"city\"].astype(str) + \", \" + cities_df[\"state_id\"].astype(str)\n",
    "\n",
    "# Assign nearest city only when it is within threshold\n",
    "df[\"vehicle_city\"] = np.where(\n",
    "    dist_km <= threshold_km,\n",
    "    cities_df.iloc[idx][\"city_full\"].values,\n",
    "    np.nan\n",
    ")\n",
    "# Remove rows with any NaN values\n",
    "# df_clean = df.dropna()\n",
    "df = df.dropna()\n",
    "\n",
    "# Check the result\n",
    "# print(df_clean.shape)  # number of rows and columns after dropping\n",
    "# print(df['vehicle_city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/8 Add a unique ID for each row\n",
    "df['row_id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Check\n",
    "# print(df[['row_id', 'vehicle_gps_latitude', 'vehicle_gps_longitude']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/9 Feature Engineering ---\n",
    "\n",
    "## 5.1 Create derived features\n",
    "# Example: combine GPS into one column\n",
    "df['vehicle_location'] = df['vehicle_gps_latitude'].astype(str) + ',' + df['vehicle_gps_longitude'].astype(str)\n",
    "\n",
    "# Example: compute delay difference in minutes\n",
    "df['eta_variation_minutes'] = df['eta_variation_hours'] * 60\n",
    "\n",
    "# Example: categorize traffic level\n",
    "df['traffic_category'] = pd.cut(\n",
    "    df['traffic_congestion_level'],\n",
    "    bins=[0, 3, 6, 10],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Example: flag if weather severity exceeds threshold\n",
    "df['severe_weather_flag'] = np.where(df['weather_condition_severity'] > 0.7, 1, 0)\n",
    "\n",
    "# ===========================================================\n",
    "print(\"\\n Feature engineering complete. New columns added:\", \n",
    "      [c for c in df.columns if 'vehicle_location' in c or 'eta_variation_minutes' in c or 'traffic_category' in c or 'severe_weather_flag' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/10. Clustering ---\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# --- Temporal features ---\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['eta_minus_actual_hours'] = df['eta_variation_hours'] - df['delivery_time_deviation']\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df = df.sort_values(['row_id', 'timestamp'])\n",
    "\n",
    "df['avg_route_congestion'] = (\n",
    "    df.groupby('route_risk_level')['traffic_congestion_level']\n",
    "      .transform(lambda x: x.rolling(5).mean())\n",
    ")\n",
    "\n",
    "# --- Clustering ---\n",
    "coords = df[['vehicle_gps_latitude', 'vehicle_gps_longitude']]\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['route_cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# --- Cluster statistics ---\n",
    "cluster_stats = (\n",
    "    df.groupby('route_cluster')\n",
    "      .agg(\n",
    "          avg_delay=('delivery_time_deviation', 'mean'),\n",
    "          avg_risk=('route_risk_level', 'mean'),\n",
    "          avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "          lat=('vehicle_gps_latitude', 'mean'),\n",
    "          lon=('vehicle_gps_longitude', 'mean')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Determine geographic labels\n",
    "lat_median = cluster_stats['lat'].median()\n",
    "lon_median = cluster_stats['lon'].median()\n",
    "\n",
    "def geo_label(row):\n",
    "    parts = []\n",
    "    # North / South\n",
    "    parts.append(\"Northern\" if row['lat'] > lat_median else \"Southern\")\n",
    "    # East / West\n",
    "    parts.append(\"Eastern\" if row['lon'] > lon_median else \"Western\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "cluster_stats['geo'] = cluster_stats.apply(geo_label, axis=1)\n",
    "\n",
    "# Categorize delay\n",
    "def delay_label(v):\n",
    "    return \"High-delay\" if v > cluster_stats['avg_delay'].median() else \"Stable-delivery\"\n",
    "\n",
    "# Categorize risk\n",
    "def risk_label(v):\n",
    "    return \"High-risk\" if v > cluster_stats['avg_risk'].median() else \"Low-risk\"\n",
    "\n",
    "# Congestion\n",
    "def congestion_label(v):\n",
    "    return \"Congested\" if v > cluster_stats['avg_congestion'].median() else \"Free-flow\"\n",
    "\n",
    "cluster_stats['delay_text'] = cluster_stats['avg_delay'].apply(delay_label)\n",
    "cluster_stats['risk_text'] = cluster_stats['avg_risk'].apply(risk_label)\n",
    "cluster_stats['cong_text'] = cluster_stats['avg_congestion'].apply(congestion_label)\n",
    "\n",
    "# FINAL NAME\n",
    "cluster_stats['cluster_name'] = (\n",
    "    cluster_stats['geo'] + \" — \" +\n",
    "    cluster_stats['delay_text'] + \", \" +\n",
    "    cluster_stats['risk_text'] + \", \" +\n",
    "    cluster_stats['cong_text']\n",
    ")\n",
    "\n",
    "# Map into df\n",
    "name_map = cluster_stats.set_index('route_cluster')['cluster_name'].to_dict()\n",
    "df['cluster_desc'] = df['route_cluster'].map(name_map)\n",
    "\n",
    "cluster_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/11. Categorical columns ---\n",
    "# Identify which columns are categorical\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "print(\"Categorical columns:\", list(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/12. Categorical columns ---\n",
    "# Categorical columns → replace NaN with most frequent value (mode)\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "print(\"\\n Missing values handled with NumPy (median/mode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/14. Final Dataset Summary ---\n",
    "print(\"\\nFinal dataset shape:\", df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "display(df.head())\n",
    "# print(df.shipping_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "# ====================================================\n",
    "\n",
    "# --- 2/1. Import Required Libraries ---\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Assume df is already loaded and cleaned from Phase 1\n",
    "print(\" DataFrame available with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40512d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/2. Route clusters visualization---\n",
    "# Ensure necessary columns exist\n",
    "required_cols = [\n",
    "    'vehicle_gps_latitude',\n",
    "    'vehicle_gps_longitude',\n",
    "    'route_cluster',\n",
    "    'delivery_time_deviation'\n",
    "]\n",
    "\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='vehicle_gps_longitude',\n",
    "    y='vehicle_gps_latitude',\n",
    "    hue='cluster_desc',      # ✔ Человеческие описания в легенде\n",
    "    size='delivery_time_deviation',\n",
    "    palette='tab10',\n",
    "    sizes=(40, 300),\n",
    "    alpha=0.8,\n",
    "    legend=\"brief\"\n",
    ")\n",
    "\n",
    "plt.title(\"Route Clusters Based on GPS Coordinates\\n(Bubble Size = Delivery Time Deviation)\", fontsize=16)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/3. Prepare stat data ---\n",
    "# Define the feature columns (independent variables)\n",
    "features = [\n",
    "    \"fuel_consumption_rate\", \"traffic_congestion_level\", \"weather_condition_severity\",\n",
    "    \"warehouse_inventory_level\", \"loading_unloading_time\", \"handling_equipment_availability\",\n",
    "    'route_cluster', \"port_congestion_level\", \"shipping_costs\", \"supplier_reliability_score\", \"\"\n",
    "    \"lead_time_days\", \"route_risk_level\", \"driver_behavior_score\", \"fatigue_monitoring_score\"\n",
    "]\n",
    "\n",
    "# Define the target column (dependent variable)\n",
    "# ===============================================\n",
    "target = \"delivery_time_deviation\"\n",
    "# ===============================================\n",
    "\n",
    "# Keep only the selected columns and remove rows with missing data\n",
    "stat_df = df[features + [target]].dropna()\n",
    "\n",
    "# Check the dataset after cleaning\n",
    "print(f\"Data after filtering: {stat_df.shape[0]:,} rows and {stat_df.shape[1]} columns\")\n",
    "\n",
    "# Display summary statistics of the cleaned dataset\n",
    "stat_df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- 2.4 Scale Numerical Features ---\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()  # default range is 0 to 1\n",
    "stat_df[features] = scaler.fit_transform(stat_df[features])\n",
    "print(stat_df[features].head())\n",
    "\n",
    "\n",
    "# Check result\n",
    "print(stat_df[features].head())\n",
    "print(\"\\nCategorical encoding and scaling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2692f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/5. Data Correlation ---\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assume stat_df has the same feature and target columns\n",
    "# -----------------------------\n",
    "# 1️ Spearman correlation (monotonic relationships)\n",
    "spearman_corr = stat_df.corr(method='spearman')[target].drop(target).sort_values(ascending=False)\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=spearman_corr.values, y=spearman_corr.index, hue=spearman_corr.index, dodge=False, palette=\"viridis\", legend=False)\n",
    "plt.title(\"Spearman Correlation with Delivery Time Deviation (stat_df)\")\n",
    "plt.xlabel(\"Spearman Correlation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 2️ Mutual Information (non-linear dependencies)\n",
    "X_stat = stat_df[features]\n",
    "y_stat = stat_df[target]\n",
    "mi = mutual_info_regression(X_stat, y_stat, random_state=42)\n",
    "mi_series = pd.Series(mi, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=mi_series.values,\n",
    "    y=mi_series.index,\n",
    "    hue=mi_series.index,   # required for palette\n",
    "    dodge=False,\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Mutual Information with Delivery Time Deviation (stat_df)\")\n",
    "plt.xlabel(\"Mutual Information\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3️ Feature importance using Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_stat, y_stat)\n",
    "rf_importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=rf_importances.values, y=rf_importances.index, hue=spearman_corr.index, dodge=False, palette=\"magma\", legend=False)\n",
    "# sns.barplot(x=rf_importances.values, y=rf_importances.index, palette=\"magma\")\n",
    "plt.title(\"Random Forest Feature Importance (stat_df)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3️ DISTRIBUTION ANALYSIS AND ANOMALY DETECTION\n",
    "# ====================================================\n",
    "# --- 3/1. Plot distribution of key numeric variables---\n",
    "\n",
    "key_vars = [\n",
    "    'delivery_time_deviation',\n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "\n",
    "key_vars = [v for v in key_vars if v in df.columns]\n",
    "print(key_vars)\n",
    "\n",
    "for col in key_vars:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/2. Detect anomalies using IQR method for each key numeric column---\n",
    "\n",
    "key_vars = [    \n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "\n",
    "\n",
    "# --- Boxplot for key numeric columns ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=stat_df[key_vars], orient='h', palette='Set2')\n",
    "plt.title(\"Boxplot of Key Numeric Variables (Outliers shown)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c234cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/3. Remove anomalies using different methods and replace the data in primary df---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "# --- Step 0: Define variables ---\n",
    "skewed_vars = ['fuel_consumption_rate', 'lead_time_days']\n",
    "robust_var = 'fuel_consumption_rate'\n",
    "standard_vars = [v for v in stat_df.columns if v not in skewed_vars]\n",
    "\n",
    "# --- Step 1: Log-transform skewed features ---\n",
    "for col in skewed_vars:\n",
    "    if col in stat_df.columns:\n",
    "        stat_df[col] = np.log1p(stat_df[col])\n",
    "\n",
    "# --- Step 2: Outlier treatment for fuel_consumption_rate ---\n",
    "Q1 = stat_df[robust_var].quantile(0.25)\n",
    "Q3 = stat_df[robust_var].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_whisker = Q1 - 1.5 * IQR\n",
    "upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "# Option 1: Remove outliers\n",
    "# stat_df = stat_df[(stat_df[robust_var] >= lower_whisker) & (stat_df[robust_var] <= upper_whisker)]\n",
    "\n",
    "# Option 2: Cap outliers (recommended to keep dataset size)\n",
    "stat_df[robust_var] = np.where(\n",
    "    stat_df[robust_var] > upper_whisker, upper_whisker,\n",
    "    np.where(stat_df[robust_var] < lower_whisker, lower_whisker, stat_df[robust_var])\n",
    ")\n",
    "\n",
    "# --- Step 3: Apply RobustScaler to fuel_consumption_rate ---\n",
    "robust_scaler = RobustScaler()\n",
    "stat_df[robust_var] = robust_scaler.fit_transform(stat_df[[robust_var]])\n",
    "\n",
    "# --- Step 4: Apply StandardScaler to other features ---\n",
    "if standard_vars:\n",
    "    standard_scaler = StandardScaler()\n",
    "    stat_df[standard_vars] = standard_scaler.fit_transform(stat_df[standard_vars])\n",
    "\n",
    "# preserve df for machine learning\n",
    "m_df = df.copy()\n",
    "\n",
    "\n",
    "# --- Replace processed columns back into primary DataFrame, excluding the target ---\n",
    "for col in stat_df.columns:\n",
    "    # if col == \"delivery_time_deviation\":\n",
    "    #     continue  # skip the target column\n",
    "    \n",
    "    if col in df.columns:\n",
    "        df.loc[stat_df.index, col] = stat_df[col]\n",
    "\n",
    "\n",
    "# --- Optional: Check the first few rows ---\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/4 Boxplot for key numeric columns ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=stat_df[key_vars], orient='h', palette='Set2')\n",
    "plt.title(\"Boxplot of Key Numeric Variables (Outliers shown)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/5 Summary statistics by cluster_desc\n",
    "summary_by_cluster = df.groupby('cluster_desc').agg(\n",
    "    avg_delay=('delivery_time_deviation', 'mean'),\n",
    "    avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "    avg_fuel=('fuel_consumption_rate', 'mean'),\n",
    "    avg_shipping=('shipping_costs', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot: Average delivery delay by cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='avg_delay',\n",
    "    y='cluster_desc',\n",
    "    hue='cluster_desc',        # set hue same as y\n",
    "    data=summary_by_cluster.sort_values('avg_delay', ascending=False),\n",
    "    palette='Reds_r',\n",
    "    dodge=False,               # avoid splitting bars\n",
    "    legend=False               # hide the extra legend\n",
    ")\n",
    "plt.title(\"Average Delivery Delay by Cluster\")\n",
    "plt.xlabel(\"Average Delivery Time Deviation (hours)\")\n",
    "plt.ylabel(\"Cluster Description\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/6. Save Cleaned Dataset ---\n",
    "df.to_csv('cleaned_logistics_data.csv', index=False)\n",
    "print(\"\\n Cleaned dataset saved as 'cleaned_logistics_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 4️ SEASONAL & REGIONAL DELAY TRENDS\n",
    "# ====================================================\n",
    "# --- 4/1. fuel consumption rate by month---\n",
    "\n",
    "#correlation between fuel consuntion rate and temperature inside\n",
    "\n",
    "\n",
    "# Convert timestamp to datetime if needed\n",
    "if not np.issubdtype(df['timestamp'].dtype, np.datetime64):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Extract time-based features\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# --- 4.1 Seasonal trends (monthly average delay)\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    monthly_delay = df.groupby('month')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=monthly_delay.index, y=monthly_delay.values, marker='o')\n",
    "    plt.title(\" Average Delivery Time Deviation by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Weekday delay trends\n",
    "\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "# Define correct order\n",
    "    weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "    # Convert to categorical with order\n",
    "    df['weekday'] = pd.Categorical(df['weekday'], categories=weekday_order, ordered=True)\n",
    "\n",
    "    # Group and sort\n",
    "    weekday_delay = df.groupby('weekday', observed=False)['delivery_time_deviation'].mean().sort_index()\n",
    "\n",
    "    # print(weekday_delay)\n",
    "\n",
    "    # weekday_delay = df.groupby('weekday')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    sns.barplot(\n",
    "    x=weekday_delay.index,\n",
    "    y=weekday_delay.values,\n",
    "    hue=weekday_delay.values,  # use values to color bars\n",
    "    palette='crest',\n",
    "    legend=False               # hide legend\n",
    "    )\n",
    "\n",
    "    plt.title(\" Average Delivery Deviation by Weekday\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Phase 2: EDA completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 5: Machine Learning Modeling\n",
    "# ====================================================\n",
    "# 5/0 FEATURE SELECTION & ENCODING\n",
    "\n",
    "# Drop non-numeric / irrelevant columns\n",
    "exclude_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname', 'vehicke_city', 'traffic_category']\n",
    "m_df = m_df.drop(columns=[c for c in exclude_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in m_df.select_dtypes(include='object').columns:\n",
    "    m_df[col] = LabelEncoder().fit_transform(m_df[col].astype(str))\n",
    "\n",
    "# Replace remaining NaNs with median\n",
    "m_df = m_df.fillna(df.median(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 DATA CLEANUP BEFORE MODELING\n",
    "\n",
    "# Drop identifier or irrelevant columns if they exist\n",
    "drop_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email', \n",
    "    'Customer Fname', 'Customer Lname', 'vehicle_city', 'weekday'\n",
    "]\n",
    "m_df = m_df.drop(columns=[c for c in drop_cols if c in m_df.columns], errors='ignore')\n",
    "\n",
    "# --- 1. Convert booleans to int\n",
    "bool_cols = m_df.select_dtypes(include=['bool']).columns\n",
    "if len(bool_cols):\n",
    "    m_df[bool_cols] = m_df[bool_cols].astype(int)\n",
    "\n",
    "# --- 2. Encode categoricals (object / category)\n",
    "cat_cols = m_df.select_dtypes(include=['object', 'category']).columns\n",
    "if len(cat_cols):\n",
    "    print(\"Encoding categorical columns:\", list(cat_cols))\n",
    "    for col in cat_cols:\n",
    "        m_df[col] = m_df[col].astype(str).astype('category').cat.codes\n",
    "\n",
    "# --- 3. Replace any infinite or missing values\n",
    "m_df = m_df.replace([np.inf, -np.inf], np.nan)\n",
    "m_df = m_df.fillna(m_df.median(numeric_only=True))\n",
    "\n",
    "# --- 4. Enforce numeric dtype globally\n",
    "# m_df = m_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "# --- 5. Verify everything is numeric\n",
    "non_numeric = m_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\" Still non-numeric columns:\", non_numeric)\n",
    "else:\n",
    "    print(\" All columns successfully converted to numeric.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a633d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 checking df\n",
    "\n",
    "# checking dataframe\n",
    "print(\"m_df shape:\", m_df.shape)\n",
    "print(\"m_df columns:\", m_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5/3 Regression Modeling ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Target column ---\n",
    "target_reg = \"delivery_time_deviation\"\n",
    "\n",
    "# --- Copy numeric features ---\n",
    "numeric_cols = m_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove the target column\n",
    "if 'delivery_time_deviation' in numeric_cols:\n",
    "    numeric_cols.remove('delivery_time_deviation')\n",
    "X = m_df[numeric_cols].copy()\n",
    "y_reg = m_df[target_reg]\n",
    "\n",
    "# --- 1. Log-transform skewed features ---\n",
    "for col in ['fuel_consumption_rate', 'lead_time_days']:\n",
    "    if col in X.columns:\n",
    "        X[col] = np.log1p(X[col])  # safe for 0 values\n",
    "\n",
    "# --- 2. Cap outliers in fuel_consumption_rate ---\n",
    "if 'fuel_consumption_rate' in X.columns:\n",
    "    Q1 = X['fuel_consumption_rate'].quantile(0.25)\n",
    "    Q3 = X['fuel_consumption_rate'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "    X['fuel_consumption_rate'] = np.where(\n",
    "        X['fuel_consumption_rate'] > upper_whisker, upper_whisker,\n",
    "        np.where(X['fuel_consumption_rate'] < lower_whisker, lower_whisker, X['fuel_consumption_rate'])\n",
    "    )\n",
    "\n",
    "# --- 3. Impute missing values ---\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# --- 4. Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# --- 5. Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# --- 6. Define regression models ---\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Train models and evaluate ---\n",
    "results_reg = {}\n",
    "print(\"Training regression models...\")\n",
    "for name in tqdm(models_reg.keys(), desc=\"Training Progress\", leave=False):\n",
    "    model = models_reg[name]\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "    print(f\"\\n{name} done.\")\n",
    "    print(f\"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R²: {r2:.3f}\")\n",
    "\n",
    "# --- Display results ---\n",
    "results_reg_df = pd.DataFrame(results_reg).T\n",
    "display(results_reg_df.style.background_gradient(cmap=\"Blues\").format(\"{:.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 5/4: Regression Modeling — Delivery Time Deviation\n",
    "# ====================================================\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- 1. Drop rows with missing target ---\n",
    "target = 'delivery_time_deviation'\n",
    "df_model = m_df.dropna(subset=[target]).copy()\n",
    "\n",
    "# --- 2. Define numeric and categorical features ---\n",
    "numeric_features = [\n",
    "    'fuel_consumption_rate', 'eta_variation_hours', 'traffic_congestion_level',\n",
    "    'warehouse_inventory_level', 'loading_unloading_time', 'port_congestion_level',\n",
    "    'shipping_costs', 'supplier_reliability_score', 'lead_time_days',\n",
    "    'historical_demand', 'iot_temperature', 'customs_clearance_time',\n",
    "    'driver_behavior_score', 'fatigue_monitoring_score', 'avg_route_congestion'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'handling_equipment_availability', 'order_fulfillment_status',\n",
    "    'weather_condition_severity', 'cargo_condition_status',\n",
    "    'route_risk_level', 'day_of_week', 'month', 'route_cluster',\n",
    "    'cluster_desc', 'year'\n",
    "]\n",
    "\n",
    "# --- 3. Exclude leaky or irrelevant features ---\n",
    "exclude_features = [\n",
    "    'disruption_likelihood_score', 'delay_probability', 'risk_classification',\n",
    "    'eta_minus_actual_hours', 'eta_variation_minutes', 'vehicle_gps_latitude',\n",
    "    'vehicle_gps_longitude', 'vehicle_location', 'row_id'\n",
    "]\n",
    "numeric_features = [f for f in numeric_features if f in df_model.columns and f not in exclude_features]\n",
    "categorical_features = [f for f in categorical_features if f in df_model.columns and f not in exclude_features]\n",
    "\n",
    "# --- 4. Prepare X and y safely ---\n",
    "X = df_model[numeric_features + categorical_features].copy()\n",
    "y = df_model[target].copy()\n",
    "assert len(X) == len(y), f\"Length mismatch: {len(X)} vs {len(y)}\"\n",
    "\n",
    "print(f\"Using {len(numeric_features)} numeric and {len(categorical_features)} categorical features\")\n",
    "\n",
    "# --- 5. Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 6. Preprocessing pipelines ---\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# --- 7. Full pipeline with Random Forest Regressor ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=10, random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 8. Train model ---\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- 9. Evaluate model ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nDelivery Time Deviation Prediction Performance:\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "\n",
    "# --- 10. Feature importance safely ---\n",
    "regressor = pipeline.named_steps['regressor']\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Numeric feature names\n",
    "numeric_names = numeric_features\n",
    "\n",
    "# Categorical feature names after one-hot encoding\n",
    "cat_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine\n",
    "all_features = list(numeric_names) + list(cat_names)\n",
    "\n",
    "# Match lengths with actual importances\n",
    "importances = regressor.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=all_features[:len(importances)]).sort_values(ascending=False)\n",
    "\n",
    "# --- 11. Plot top 10 feature importances ---\n",
    "plt.figure(figsize=(10,6))\n",
    "feat_imp.head(10).plot(kind='barh', color='steelblue')\n",
    "plt.title(\"Top 10 Feature Importances — Delivery Time Deviation\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autowini_venv)",
   "language": "python",
   "name": "autowini_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
