{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 1: Data Preparation\n",
    "# Project: Predictive Analytics for Supply Chain Optimization\n",
    "# ====================================================\n",
    "\n",
    "# --- 1. Import Required Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6910ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='latin-1')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Show list of columns\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Basic Data Overview ---\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "# print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nMissing values summary:\\n\", df.isna().sum())\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fef0ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose range and count\n",
    "start_date = pd.to_datetime(\"2024-01-01\")\n",
    "end_date = pd.to_datetime(\"2024-12-31\")\n",
    "\n",
    "# Replace timestamp column with random dates\n",
    "df['timestamp'] = pd.to_datetime(\n",
    "    np.random.randint(\n",
    "        start_date.value // 10**9,\n",
    "        end_date.value // 10**9,\n",
    "        len(df)\n",
    "    ),\n",
    "    unit='s'\n",
    ")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Handle Missing Values with NumPy ---\n",
    "# Identify numeric columns, but exclude 'timestamp'\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != 'timestamp']\n",
    "\n",
    "# Identify categorical columns\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Impute numeric columns with median\n",
    "for col in num_cols:\n",
    "    median_val = np.nanmedian(df[col])\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# If 'timestamp' is datetime and has missing values, optionally impute separately\n",
    "if df['timestamp'].isna().any():\n",
    "    # Replace NaT with some default, e.g., first date\n",
    "    df['timestamp'] = df['timestamp'].fillna(df['timestamp'].min())\n",
    "\n",
    "\n",
    "# print(df['timestamp'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7be9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where vehicle_gps_latitude >= 35\n",
    "df = df[df['vehicle_gps_latitude'] >= 35]\n",
    "df = df[df['vehicle_gps_longitude'] >= -110]\n",
    "df = df[df['vehicle_gps_longitude'] <= -85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0053eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8      40.0, -91.0\n",
      "17    42.0, -101.0\n",
      "25     39.0, -98.0\n",
      "33     45.0, -91.0\n",
      "35     35.0, -99.0\n",
      "Name: vehicle_city, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create vehicle_city column based on coordinates\n",
    "# Suppose your df has Latitude and Longitude as numeric columns\n",
    "# Round coordinates to 0 decimal places (~100 km grid)\n",
    "df['lat_bin'] = df['vehicle_gps_latitude'].round(0)\n",
    "df['lon_bin'] = df['vehicle_gps_longitude'].round(0)\n",
    "\n",
    "# Create a pseudo city label\n",
    "df['vehicle_city'] = df['lat_bin'].astype(str) + \", \" + df['lon_bin'].astype(str)\n",
    "\n",
    "# Optional: check unique values\n",
    "print(df['vehicle_city'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d21c3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a unique ID for each row\n",
    "df['row_id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Check\n",
    "# print(df[['row_id', 'vehicle_gps_latitude', 'vehicle_gps_longitude']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc94d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Feature engineering complete. New columns added: ['vehicle_location', 'eta_variation_minutes', 'traffic_category', 'severe_weather_flag']\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Feature Engineering ---\n",
    "\n",
    "## 5.1 Create derived features\n",
    "# Example: combine GPS into one column\n",
    "df['vehicle_location'] = df['vehicle_gps_latitude'].astype(str) + ',' + df['vehicle_gps_longitude'].astype(str)\n",
    "\n",
    "# Example: compute delay difference in minutes\n",
    "df['eta_variation_minutes'] = df['eta_variation_hours'] * 60\n",
    "\n",
    "# Example: categorize traffic level\n",
    "df['traffic_category'] = pd.cut(\n",
    "    df['traffic_congestion_level'],\n",
    "    bins=[0, 3, 6, 10],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Example: flag if weather severity exceeds threshold\n",
    "df['severe_weather_flag'] = np.where(df['weather_condition_severity'] > 7, 1, 0)\n",
    "\n",
    "print(\"\\n Feature engineering complete. New columns added:\", \n",
    "      [c for c in df.columns if 'vehicle_location' in c or 'eta_variation_minutes' in c or 'traffic_category' in c or 'severe_weather_flag' in c])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5eacfb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['risk_classification', 'vehicle_city', 'vehicle_location', 'traffic_category']\n"
     ]
    }
   ],
   "source": [
    "# Identify which columns are categorical\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "print(\"Categorical columns:\", list(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb471b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Missing values handled with NumPy (median/mode).\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns ‚Üí replace NaN with most frequent value (mode)\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "print(\"\\n Missing values handled with NumPy (median/mode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0fc3e368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cleaned dataset saved as 'cleaned_logistics_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Save Cleaned Dataset ---\n",
    "df.to_csv('cleaned_logistics_data.csv', index=False)\n",
    "print(\"\\n Cleaned dataset saved as 'cleaned_logistics_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21408231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    risk_classification  vehicle_city  vehicle_location  traffic_category\n",
      "8                     2           147              1629                 0\n",
      "17                    0           183              2357                 1\n",
      "25                    0           128              1398                 2\n",
      "33                    0           277              3191                 0\n",
      "35                    2            25                84                 0\n",
      "\n",
      " Categorical encoding complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Encode Categorical Features ---\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Check result\n",
    "print(df[cat_cols].head())\n",
    "\n",
    "\n",
    "print(\"\\n Categorical encoding complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71ee6f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Removed 0 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Remove Duplicates ---\n",
    "initial_shape = df.shape\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"\\n Removed {initial_shape[0] - df.shape[0]} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 8. Scale Numerical Features ---\n",
    "# scaler = StandardScaler()\n",
    "# df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "# print(\"\\n Numerical features scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6485354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Numerical features scaled with Min-Max Scaler.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "print(\"\\n Numerical features scaled with Min-Max Scaler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d407b0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset shape: (5578, 34)\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>vehicle_gps_latitude</th>\n",
       "      <th>vehicle_gps_longitude</th>\n",
       "      <th>fuel_consumption_rate</th>\n",
       "      <th>eta_variation_hours</th>\n",
       "      <th>traffic_congestion_level</th>\n",
       "      <th>warehouse_inventory_level</th>\n",
       "      <th>loading_unloading_time</th>\n",
       "      <th>handling_equipment_availability</th>\n",
       "      <th>order_fulfillment_status</th>\n",
       "      <th>weather_condition_severity</th>\n",
       "      <th>port_congestion_level</th>\n",
       "      <th>shipping_costs</th>\n",
       "      <th>supplier_reliability_score</th>\n",
       "      <th>lead_time_days</th>\n",
       "      <th>historical_demand</th>\n",
       "      <th>iot_temperature</th>\n",
       "      <th>cargo_condition_status</th>\n",
       "      <th>route_risk_level</th>\n",
       "      <th>customs_clearance_time</th>\n",
       "      <th>driver_behavior_score</th>\n",
       "      <th>fatigue_monitoring_score</th>\n",
       "      <th>disruption_likelihood_score</th>\n",
       "      <th>delay_probability</th>\n",
       "      <th>risk_classification</th>\n",
       "      <th>delivery_time_deviation</th>\n",
       "      <th>lat_bin</th>\n",
       "      <th>lon_bin</th>\n",
       "      <th>vehicle_city</th>\n",
       "      <th>row_id</th>\n",
       "      <th>vehicle_location</th>\n",
       "      <th>eta_variation_minutes</th>\n",
       "      <th>traffic_category</th>\n",
       "      <th>severe_weather_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-10-05 01:09:11</td>\n",
       "      <td>0.319122</td>\n",
       "      <td>0.764121</td>\n",
       "      <td>0.045378</td>\n",
       "      <td>0.994125</td>\n",
       "      <td>0.961316</td>\n",
       "      <td>0.109817</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.049305</td>\n",
       "      <td>0.183042</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.845226</td>\n",
       "      <td>0.427177</td>\n",
       "      <td>0.242227</td>\n",
       "      <td>0.344144</td>\n",
       "      <td>0.994265</td>\n",
       "      <td>8.052130e-07</td>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.715884</td>\n",
       "      <td>0.371081</td>\n",
       "      <td>0.876879</td>\n",
       "      <td>0.996431</td>\n",
       "      <td>0.321187</td>\n",
       "      <td>0.223527</td>\n",
       "      <td>2</td>\n",
       "      <td>0.276943</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>1629</td>\n",
       "      <td>297.532504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-03-31 00:10:04</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.378335</td>\n",
       "      <td>0.066739</td>\n",
       "      <td>0.999234</td>\n",
       "      <td>0.147293</td>\n",
       "      <td>0.055212</td>\n",
       "      <td>0.860251</td>\n",
       "      <td>0.286445</td>\n",
       "      <td>0.845133</td>\n",
       "      <td>0.052157</td>\n",
       "      <td>0.130621</td>\n",
       "      <td>0.014826</td>\n",
       "      <td>0.018891</td>\n",
       "      <td>0.243462</td>\n",
       "      <td>0.035190</td>\n",
       "      <td>3.633830e-08</td>\n",
       "      <td>0.135197</td>\n",
       "      <td>0.498691</td>\n",
       "      <td>0.607153</td>\n",
       "      <td>0.675503</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0.997020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-101.0</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>2357</td>\n",
       "      <td>299.678144</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024-08-16 09:07:37</td>\n",
       "      <td>0.270626</td>\n",
       "      <td>0.482519</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.980595</td>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.061652</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.263769</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.975974</td>\n",
       "      <td>0.301094</td>\n",
       "      <td>0.626989</td>\n",
       "      <td>0.362142</td>\n",
       "      <td>0.995295</td>\n",
       "      <td>0.547009</td>\n",
       "      <td>4.909631e-01</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.204691</td>\n",
       "      <td>0.640925</td>\n",
       "      <td>0.096113</td>\n",
       "      <td>0.730927</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.926330</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>1398</td>\n",
       "      <td>291.849939</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2024-09-02 15:56:34</td>\n",
       "      <td>0.648524</td>\n",
       "      <td>0.761405</td>\n",
       "      <td>0.993794</td>\n",
       "      <td>0.269726</td>\n",
       "      <td>0.869466</td>\n",
       "      <td>0.022763</td>\n",
       "      <td>0.216089</td>\n",
       "      <td>0.600112</td>\n",
       "      <td>0.967194</td>\n",
       "      <td>0.784470</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.060724</td>\n",
       "      <td>0.981911</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.994459</td>\n",
       "      <td>4.217911e-01</td>\n",
       "      <td>0.010098</td>\n",
       "      <td>0.690826</td>\n",
       "      <td>0.144589</td>\n",
       "      <td>0.710606</td>\n",
       "      <td>0.219789</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>0.177602</td>\n",
       "      <td>0</td>\n",
       "      <td>0.267472</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>277</td>\n",
       "      <td>4</td>\n",
       "      <td>3191</td>\n",
       "      <td>-6.714285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2024-05-18 20:03:39</td>\n",
       "      <td>0.012835</td>\n",
       "      <td>0.459613</td>\n",
       "      <td>0.033852</td>\n",
       "      <td>0.245045</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>0.967045</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.029390</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.152799</td>\n",
       "      <td>0.738910</td>\n",
       "      <td>0.535708</td>\n",
       "      <td>0.995374</td>\n",
       "      <td>0.538911</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>2.857528e-01</td>\n",
       "      <td>0.422151</td>\n",
       "      <td>0.650865</td>\n",
       "      <td>0.073794</td>\n",
       "      <td>0.103517</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.690875</td>\n",
       "      <td>0.193168</td>\n",
       "      <td>2</td>\n",
       "      <td>0.770661</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>-17.079914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  vehicle_gps_latitude  vehicle_gps_longitude  \\\n",
       "8  2024-10-05 01:09:11              0.319122               0.764121   \n",
       "17 2024-03-31 00:10:04              0.470999               0.378335   \n",
       "25 2024-08-16 09:07:37              0.270626               0.482519   \n",
       "33 2024-09-02 15:56:34              0.648524               0.761405   \n",
       "35 2024-05-18 20:03:39              0.012835               0.459613   \n",
       "\n",
       "    fuel_consumption_rate  eta_variation_hours  traffic_congestion_level  \\\n",
       "8                0.045378             0.994125                  0.961316   \n",
       "17               0.066739             0.999234                  0.147293   \n",
       "25               0.000792             0.980595                  0.473047   \n",
       "33               0.993794             0.269726                  0.869466   \n",
       "35               0.033852             0.245045                  0.973184   \n",
       "\n",
       "    warehouse_inventory_level  loading_unloading_time  \\\n",
       "8                    0.109817                0.020500   \n",
       "17                   0.055212                0.860251   \n",
       "25                   0.061652                0.759500   \n",
       "33                   0.022763                0.216089   \n",
       "35                   0.967045                0.003881   \n",
       "\n",
       "    handling_equipment_availability  order_fulfillment_status  \\\n",
       "8                          0.049305                  0.183042   \n",
       "17                         0.286445                  0.845133   \n",
       "25                         0.263769                  0.999889   \n",
       "33                         0.600112                  0.967194   \n",
       "35                         0.029390                  0.296500   \n",
       "\n",
       "    weather_condition_severity  port_congestion_level  shipping_costs  \\\n",
       "8                     0.868000               0.845226        0.427177   \n",
       "17                    0.052157               0.130621        0.014826   \n",
       "25                    0.975974               0.301094        0.626989   \n",
       "33                    0.784470               0.999113        0.060724   \n",
       "35                    0.152799               0.738910        0.535708   \n",
       "\n",
       "    supplier_reliability_score  lead_time_days  historical_demand  \\\n",
       "8                     0.242227        0.344144           0.994265   \n",
       "17                    0.018891        0.243462           0.035190   \n",
       "25                    0.362142        0.995295           0.547009   \n",
       "33                    0.981911        0.001604           0.994459   \n",
       "35                    0.995374        0.538911           0.995789   \n",
       "\n",
       "    iot_temperature  cargo_condition_status  route_risk_level  \\\n",
       "8      8.052130e-07                0.914182          0.715884   \n",
       "17     3.633830e-08                0.135197          0.498691   \n",
       "25     4.909631e-01                0.000054          0.204691   \n",
       "33     4.217911e-01                0.010098          0.690826   \n",
       "35     2.857528e-01                0.422151          0.650865   \n",
       "\n",
       "    customs_clearance_time  driver_behavior_score  fatigue_monitoring_score  \\\n",
       "8                 0.371081               0.876879                  0.996431   \n",
       "17                0.607153               0.675503                  0.513400   \n",
       "25                0.640925               0.096113                  0.730927   \n",
       "33                0.144589               0.710606                  0.219789   \n",
       "35                0.073794               0.103517                  0.998147   \n",
       "\n",
       "    disruption_likelihood_score  delay_probability  risk_classification  \\\n",
       "8                      0.321187           0.223527                    2   \n",
       "17                     0.989861           0.997020                    0   \n",
       "25                     0.999986           0.926330                    0   \n",
       "33                     0.998641           0.177602                    0   \n",
       "35                     0.690875           0.193168                    2   \n",
       "\n",
       "    delivery_time_deviation  lat_bin  lon_bin  vehicle_city  row_id  \\\n",
       "8                  0.276943     40.0    -91.0           147       1   \n",
       "17                 0.063334     42.0   -101.0           183       2   \n",
       "25                 0.999903     39.0    -98.0           128       3   \n",
       "33                 0.267472     45.0    -91.0           277       4   \n",
       "35                 0.770661     35.0    -99.0            25       5   \n",
       "\n",
       "    vehicle_location  eta_variation_minutes  traffic_category  \\\n",
       "8               1629             297.532504                 0   \n",
       "17              2357             299.678144                 1   \n",
       "25              1398             291.849939                 2   \n",
       "33              3191              -6.714285                 0   \n",
       "35                84             -17.079914                 0   \n",
       "\n",
       "    severe_weather_flag  \n",
       "8                     0  \n",
       "17                    0  \n",
       "25                    0  \n",
       "33                    0  \n",
       "35                    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 9. Final Dataset Summary ---\n",
    "print(\"\\nFinal dataset shape:\", df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "display(df.head())\n",
    "# print(df.shipping_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by='shipping_costs')\n",
    "print(df_sorted.shipping_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d38b1f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DataFrame available with shape: (5578, 34)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "# ====================================================\n",
    "\n",
    "# --- 1. Import Required Libraries ---\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Assume df is already loaded and cleaned from Phase 1\n",
    "print(\" DataFrame available with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make  a histogram of delivery time deviation in hours\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['delivery_time_deviation'], bins=30, kde=True, color='blue')\n",
    "plt.title('Histogram of Delivery Time Deviation (Hours)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CORRELATION ANALYSIS\n",
    "# ====================================================\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns #list of all numeric column names.\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Display top correlated features with delivery deviation\n",
    "target = 'delivery_time_deviation'\n",
    "if target in corr_matrix.columns:\n",
    "    print(\"\\n Top 10 correlations with delivery_time_deviation:\")\n",
    "    print(corr_matrix[target].abs().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n",
    "#sns.scatterplot(x='traffic_congestion_level_num', y='delivery_time_deviation', data=df)\n",
    "\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3Ô∏è DISTRIBUTION ANALYSIS AND ANOMALY DETECTION\n",
    "# ====================================================\n",
    "\n",
    "# Plot distribution of key numeric variables\n",
    "key_vars = [\n",
    "    'delivery_time_deviation',\n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "key_vars = [v for v in key_vars if v in df.columns]\n",
    "\n",
    "for col in key_vars:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using IQR method for each key numeric column\n",
    "def detect_outliers_iqr(series):\n",
    "    q1, q3 = np.percentile(series.dropna(), [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    return ((series < lower) | (series > upper)).sum()\n",
    "\n",
    "outlier_summary = {col: detect_outliers_iqr(df[col]) for col in key_vars}\n",
    "print(\"\\nüîé Outlier count per key variable:\")\n",
    "print(pd.Series(outlier_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 4Ô∏è SEASONAL & REGIONAL DELAY TRENDS\n",
    "# ====================================================\n",
    "#fuel consumption rate by month\n",
    "\n",
    "#correlation between fuel consuntion rate and temperature inside\n",
    "\n",
    "\n",
    "\n",
    "# Convert timestamp to datetime if needed\n",
    "if not np.issubdtype(df['timestamp'].dtype, np.datetime64):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Extract time-based features\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# --- 4.1 Seasonal trends (monthly average delay)\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    monthly_delay = df.groupby('month')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=monthly_delay.index, y=monthly_delay.values, marker='o')\n",
    "    plt.title(\" Average Delivery Time Deviation by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 Regional trends (by city or location if available)\n",
    "if 'Customer City' in df.columns:\n",
    "    regional_delay = df.groupby('Customer City')['delivery_time_deviation'].mean().sort_values(ascending=False).head(10)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=regional_delay.values, y=regional_delay.index, palette='coolwarm')\n",
    "    plt.title(\" Top 10 Cities by Average Delivery Deviation\")\n",
    "    plt.xlabel(\"Average Delay\")\n",
    "    plt.ylabel(\"City\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n No 'Customer City' column found for regional analysis. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Weekday delay trends\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    weekday_delay = df.groupby('weekday')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=weekday_delay.index, y=weekday_delay.values, palette='crest')\n",
    "    plt.title(\" Average Delivery Deviation by Weekday\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Phase 2: EDA completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 3: Machine Learning Modeling\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "\n",
    "# Assume df is already loaded and cleaned from previous phases\n",
    "print(\" DataFrame ready with shape:\", df.shape)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2Ô∏è FEATURE SELECTION & ENCODING\n",
    "# ====================================================\n",
    "\n",
    "# Drop non-numeric / irrelevant columns\n",
    "exclude_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname']\n",
    "df = df.drop(columns=[c for c in exclude_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Replace remaining NaNs with median\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# üîß DATA CLEANUP BEFORE MODELING ‚Äî FINAL SAFE VERSION\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# Work on a copy to preserve df\n",
    "data = df.copy()\n",
    "\n",
    "# Drop identifier or irrelevant columns if they exist\n",
    "drop_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email',\n",
    "    'Customer Fname', 'Customer Lname'\n",
    "]\n",
    "data = data.drop(columns=[c for c in drop_cols if c in data.columns], errors='ignore')\n",
    "\n",
    "# --- 1. Convert booleans to int\n",
    "bool_cols = data.select_dtypes(include=['bool']).columns\n",
    "if len(bool_cols):\n",
    "    data[bool_cols] = data[bool_cols].astype(int)\n",
    "\n",
    "# --- 2. Encode categoricals (object / category)\n",
    "cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "if len(cat_cols):\n",
    "    print(\"Encoding categorical columns:\", list(cat_cols))\n",
    "    for col in cat_cols:\n",
    "        data[col] = data[col].astype(str).astype('category').cat.codes\n",
    "\n",
    "# --- 3. Replace any infinite or missing values\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.fillna(data.median(numeric_only=True))\n",
    "\n",
    "# --- 4. Enforce numeric dtype globally\n",
    "data = data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# --- 5. Verify everything is numeric\n",
    "non_numeric = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\" Still non-numeric columns:\", non_numeric)\n",
    "else:\n",
    "    print(\" All columns successfully converted to numeric.\")\n",
    "\n",
    "#  Ready for modeling\n",
    "df = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 3: Model Training with Progress Bar ---\n",
    "\n",
    "#!pip install tqdm --quiet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from tqdm.notebook import tqdm  # progress bar for Jupyter\n",
    "\n",
    "# --- Example target columns ---\n",
    "target_reg = \"delivery_time_deviation\"\n",
    "target_cls = \"risk_classification\"\n",
    "\n",
    "# --- Ensure numeric features only ---\n",
    "X = df.select_dtypes(include=[np.number])\n",
    "y_reg = df[target_reg]\n",
    "y_cls = df[target_cls].astype(str)  # ensure string for classification\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Models ---\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Results dict ---\n",
    "results_reg = {}\n",
    "\n",
    "# --- Model training with progress bar ---\n",
    "print(\" Training regression models...\")\n",
    "for name in tqdm(models_reg.keys(), desc=\"Training Progress\", leave=False):\n",
    "    model = models_reg[name]\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "    print(f\"\\n {name} done.\")\n",
    "    print(f\"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "# --- Display summary ---\n",
    "results_reg_df = pd.DataFrame(results_reg).T\n",
    "display(results_reg_df.style.background_gradient(cmap=\"Blues\").format(\"{:.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global storage for trained models\n",
    "trained_models = {\n",
    "    \"regression\": {},\n",
    "    \"classification\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 3: Machine Learning Modeling (Optimized)\n",
    "# ====================================================\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "\n",
    "# --- 1Ô∏è Preprocessing: ensure numeric ---\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Drop identifier columns\n",
    "drop_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname']\n",
    "df_ml = df_ml.drop(columns=[c for c in drop_cols if c in df_ml.columns], errors='ignore')\n",
    "\n",
    "# Encode booleans\n",
    "bool_cols = df_ml.select_dtypes(include='bool').columns\n",
    "df_ml[bool_cols] = df_ml[bool_cols].astype(int)\n",
    "\n",
    "# Encode categorical columns\n",
    "cat_cols = df_ml.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df_ml[col] = LabelEncoder().fit_transform(df_ml[col].astype(str))\n",
    "\n",
    "# Fill NaNs\n",
    "df_ml = df_ml.fillna(df_ml.median(numeric_only=True))\n",
    "\n",
    "print(\" Preprocessing complete. Data ready for modeling.\")\n",
    "\n",
    "# --- 2Ô∏è Regression: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\n Regression: delivery_time_deviation\")\n",
    "\n",
    "    X_reg = df_ml.drop(columns=['delivery_time_deviation'])\n",
    "    y_reg = df_ml['delivery_time_deviation']\n",
    "\n",
    "    # --- Optional: sample for speed ---\n",
    "    sample_size = 5000\n",
    "    if len(X_reg) > sample_size:\n",
    "        X_reg = X_reg.sample(sample_size, random_state=42)\n",
    "        y_reg = y_reg.loc[X_reg.index]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_reg = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest Regressor\": RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "    }\n",
    "\n",
    "    results_reg = {}\n",
    "    for name in tqdm(models_reg.keys(), desc=\"Regression Models\", leave=False):\n",
    "        model = models_reg[name]\n",
    "        start = pd.Timestamp.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "        print(f\"\\n {name} done in {elapsed:.2f} sec | MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "        # Feature importance for Random Forest\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feat_imp = pd.Series(model.feature_importances_, index=X_reg.columns)\n",
    "            feat_imp.nlargest(10).plot(kind='barh', figsize=(8,4), title=f\"Top 10 Feature Importances ({name})\")\n",
    "            plt.show()\n",
    "\n",
    "# --- 3Ô∏è Classification: risk_classification & delay_probability ---\n",
    "def run_classification(target_col):\n",
    "    print(f\"\\n Classification: {target_col}\")\n",
    "    if target_col not in df_ml.columns:\n",
    "        print(f\" Column '{target_col}' not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    X = df_ml.drop(columns=[target_col])\n",
    "    y = df_ml[target_col]\n",
    "\n",
    "    # Optional sampling\n",
    "    sample_size = 5000\n",
    "    if len(X) > sample_size:\n",
    "        X = X.sample(sample_size, random_state=42)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    # Encode target\n",
    "    y = LabelEncoder().fit_transform(y.astype(str))\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_cls = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "    }\n",
    "\n",
    "    for name in tqdm(models_cls.keys(), desc=f\"{target_col} Models\", leave=False):\n",
    "        model = models_cls[name]\n",
    "        start = pd.Timestamp.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        # --- Save trained model globally ---\n",
    "        trained_models['classification'][f\"{target}_{name}\"] = model\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(f\"\\n {name} done in {elapsed:.2f} sec | Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Run classification tasks\n",
    "for target in ['risk_classification', 'delay_probability']:\n",
    "    run_classification(target)\n",
    "\n",
    "print(\"\\n Phase 3: Machine Learning Modeling completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2942901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 4: Insights & Recommendations\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "print(\" Phase 4: Interpreting model outputs and generating recommendations\")\n",
    "\n",
    "# --- 1Ô∏è Regression Insights: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\nüîπ Regression Insights: Top drivers of delivery time deviation\")\n",
    "\n",
    "    # Use the last trained Random Forest model from Phase 3\n",
    "    rf_model = models_reg.get(\"Random Forest Regressor\", None)\n",
    "\n",
    "    if rf_model is not None:\n",
    "        # Feature importance\n",
    "        feat_imp = pd.Series(rf_model.feature_importances_, index=X_reg.columns).sort_values(ascending=False)\n",
    "        top_features = feat_imp.head(10)\n",
    "        print(\"\\nTop 10 features driving delivery time deviation:\")\n",
    "        display(top_features)\n",
    "\n",
    "        # Visualize\n",
    "        # sns.barplot(\n",
    "        #     x=top_features.values,\n",
    "        #     y=top_features.index,\n",
    "        #     hue=top_features.index,     # color by the same variable\n",
    "        #     palette=\"viridis\",\n",
    "        #     dodge=False,\n",
    "        #     legend=False\n",
    "        # )\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\n",
    "        plt.title(\"Top 10 Feature Importances ‚Äî Delivery Time Deviation\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features.index:\n",
    "            print(f\"- Monitor and optimize {feat} to reduce delivery time deviations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a194042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Insights ---\n",
    "for target in ['risk_classification', 'delay_probability']:\n",
    "    print(f\"\\nüîπ Classification Insights: {target}\")\n",
    "\n",
    "    # Access the trained Random Forest Classifier from the global dictionary\n",
    "    rf_model_cls = trained_models['classification'].get(f\"{target}_Random Forest Classifier\", None)\n",
    "\n",
    "    if rf_model_cls is not None:\n",
    "        # Ensure we use the same features as in training\n",
    "        X_train_cls = df_ml.select_dtypes(include=[np.number]).drop(columns=[target], errors='ignore')\n",
    "\n",
    "        # Feature importance\n",
    "        feat_imp_cls = pd.Series(rf_model_cls.feature_importances_, index=X_train_cls.columns).sort_values(ascending=False)\n",
    "        top_features_cls = feat_imp_cls.head(10)\n",
    "\n",
    "        print(\"\\nTop 10 features driving classification outcome:\")\n",
    "        display(top_features_cls)\n",
    "\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(x=top_features_cls.values, y=top_features_cls.index, palette=\"magma\")\n",
    "        plt.title(f\"Top 10 Feature Importances ‚Äî {target}\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features_cls.index:\n",
    "            print(f\"- Investigate and improve {feat} to reduce {target.replace('_',' ')} risks.\")\n",
    "    else:\n",
    "        print(f\" No trained model found for {target}, skipping.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autowini_venv)",
   "language": "python",
   "name": "autowini_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
