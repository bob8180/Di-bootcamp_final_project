{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 1: Data Preparation\n",
    "# Project: Predictive Analytics for Supply Chain Optimization\n",
    "# ====================================================\n",
    "\n",
    "# --- 1/1. Import Required Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6910ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/2. Import data ---\n",
    "try:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='utf-8')\n",
    "  cities_df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\uscities.csv\")\n",
    "except UnicodeDecodeError:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='latin-1')\n",
    "  cities_df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\uscities.csv\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "\n",
    "# Preview\n",
    "# df.head()\n",
    "# df_city.head()\n",
    "\n",
    "\n",
    "# Show list of columns\n",
    "# print(df_city.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/3. Basic Data Overview ---\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "# print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nMissing values summary:\\n\", df.isna().sum())\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/4. Timestamp for testing ---\n",
    "# Choose range and count\n",
    "start_date = pd.to_datetime(\"2023-01-01\")\n",
    "end_date = pd.to_datetime(\"2024-12-31\")\n",
    "\n",
    "# Replace timestamp column with random dates\n",
    "df['timestamp'] = pd.to_datetime(\n",
    "    np.random.randint(\n",
    "        start_date.value // 10**9,\n",
    "        end_date.value // 10**9,\n",
    "        len(df)\n",
    "    ),\n",
    "    unit='s'\n",
    ")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/5. Handle Missing Values with NumPy ---\n",
    "# Identify numeric columns, but exclude 'timestamp'\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != 'timestamp']\n",
    "\n",
    "# Identify categorical columns\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Impute numeric columns with median\n",
    "for col in num_cols:\n",
    "    median_val = np.nanmedian(df[col])\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# If 'timestamp' is datetime and has missing values, optionally impute separately\n",
    "if df['timestamp'].isna().any():\n",
    "    # Replace NaT with some default, e.g., first date\n",
    "    df['timestamp'] = df['timestamp'].fillna(df['timestamp'].min())\n",
    "\n",
    "\n",
    "# print(df['timestamp'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 1/6. Narrow search field ---\n",
    "\n",
    "# # Keep only rows where vehicle_gps_latitude >= 35\n",
    "# df = df[df['vehicle_gps_latitude'] >= 35]\n",
    "# df = df[df['vehicle_gps_longitude'] >= -110]\n",
    "# df = df[df['vehicle_gps_longitude'] <= -85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/7 Nearest City Assignment Using KDTree ---\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Example: your cities DataFrame\n",
    "# cities = pd.read_csv(\"uscities.csv\")\n",
    "# Keep only necessary columns\n",
    "cities = cities_df[['city', 'state_id', 'lat', 'lng']]\n",
    "\n",
    "# Build KDTree\n",
    "city_coords = cities_df[['lat', 'lng']].values\n",
    "tree = cKDTree(city_coords)\n",
    "\n",
    "# Vehicle coordinates\n",
    "vehicle_coords = df[['vehicle_gps_latitude', 'vehicle_gps_longitude']].values\n",
    "\n",
    "# Query nearest city (k=1 for nearest)\n",
    "distances, idx = tree.query(vehicle_coords, k=1)\n",
    "\n",
    "# Optional: compute approximate distance in km (using Haversine)\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of earth in km\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Compute distances for all rows\n",
    "vehicle_lat = df['vehicle_gps_latitude'].values\n",
    "vehicle_lon = df['vehicle_gps_longitude'].values\n",
    "city_lat = cities_df.iloc[idx]['lat'].values\n",
    "city_lon = cities_df.iloc[idx]['lng'].values\n",
    "\n",
    "dist_km = haversine(vehicle_lat, vehicle_lon, city_lat, city_lon)\n",
    "\n",
    "# Set threshold, e.g., 50 km\n",
    "threshold_km = 50\n",
    "\n",
    "# Assign vehicle_city only if within threshold\n",
    "# Pre-build city + state\n",
    "cities_df[\"city_full\"] = cities_df[\"city\"].astype(str) + \", \" + cities_df[\"state_id\"].astype(str)\n",
    "\n",
    "# Assign nearest city only when it is within threshold\n",
    "df[\"vehicle_city\"] = np.where(\n",
    "    dist_km <= threshold_km,\n",
    "    cities_df.iloc[idx][\"city_full\"].values,\n",
    "    np.nan\n",
    ")\n",
    "# Remove rows with any NaN values\n",
    "# df_clean = df.dropna()\n",
    "df = df.dropna()\n",
    "\n",
    "# Check the result\n",
    "# print(df_clean.shape)  # number of rows and columns after dropping\n",
    "# print(df['vehicle_city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/8 Add a unique ID for each row\n",
    "df['row_id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Check\n",
    "# print(df[['row_id', 'vehicle_gps_latitude', 'vehicle_gps_longitude']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/9 Feature Engineering ---\n",
    "\n",
    "## 5.1 Create derived features\n",
    "# Example: combine GPS into one column\n",
    "df['vehicle_location'] = df['vehicle_gps_latitude'].astype(str) + ',' + df['vehicle_gps_longitude'].astype(str)\n",
    "\n",
    "# Example: compute delay difference in minutes\n",
    "df['eta_variation_minutes'] = df['eta_variation_hours'] * 60\n",
    "\n",
    "# Example: categorize traffic level\n",
    "df['traffic_category'] = pd.cut(\n",
    "    df['traffic_congestion_level'],\n",
    "    bins=[0, 3, 6, 10],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Example: flag if weather severity exceeds threshold\n",
    "df['severe_weather_flag'] = np.where(df['weather_condition_severity'] > 0.7, 1, 0)\n",
    "\n",
    "# ===========================================================\n",
    "print(\"\\n Feature engineering complete. New columns added:\", \n",
    "      [c for c in df.columns if 'vehicle_location' in c or 'eta_variation_minutes' in c or 'traffic_category' in c or 'severe_weather_flag' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/10. Clustering ---\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# --- Temporal features ---\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['eta_minus_actual_hours'] = df['eta_variation_hours'] - df['delivery_time_deviation']\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df = df.sort_values(['row_id', 'timestamp'])\n",
    "\n",
    "df['avg_route_congestion'] = (\n",
    "    df.groupby('route_risk_level')['traffic_congestion_level']\n",
    "      .transform(lambda x: x.rolling(5).mean())\n",
    ")\n",
    "\n",
    "# --- Clustering ---\n",
    "coords = df[['vehicle_gps_latitude', 'vehicle_gps_longitude']]\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['route_cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# --- Cluster statistics ---\n",
    "cluster_stats = (\n",
    "    df.groupby('route_cluster')\n",
    "      .agg(\n",
    "          avg_delay=('delivery_time_deviation', 'mean'),\n",
    "          avg_risk=('route_risk_level', 'mean'),\n",
    "          avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "          lat=('vehicle_gps_latitude', 'mean'),\n",
    "          lon=('vehicle_gps_longitude', 'mean')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Determine geographic labels\n",
    "lat_median = cluster_stats['lat'].median()\n",
    "lon_median = cluster_stats['lon'].median()\n",
    "\n",
    "def geo_label(row):\n",
    "    parts = []\n",
    "    # North / South\n",
    "    parts.append(\"Northern\" if row['lat'] > lat_median else \"Southern\")\n",
    "    # East / West\n",
    "    parts.append(\"Eastern\" if row['lon'] > lon_median else \"Western\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "cluster_stats['geo'] = cluster_stats.apply(geo_label, axis=1)\n",
    "\n",
    "# Categorize delay\n",
    "def delay_label(v):\n",
    "    return \"High-delay\" if v > cluster_stats['avg_delay'].median() else \"Stable-delivery\"\n",
    "\n",
    "# Categorize risk\n",
    "def risk_label(v):\n",
    "    return \"High-risk\" if v > cluster_stats['avg_risk'].median() else \"Low-risk\"\n",
    "\n",
    "# Congestion\n",
    "def congestion_label(v):\n",
    "    return \"Congested\" if v > cluster_stats['avg_congestion'].median() else \"Free-flow\"\n",
    "\n",
    "cluster_stats['delay_text'] = cluster_stats['avg_delay'].apply(delay_label)\n",
    "cluster_stats['risk_text'] = cluster_stats['avg_risk'].apply(risk_label)\n",
    "cluster_stats['cong_text'] = cluster_stats['avg_congestion'].apply(congestion_label)\n",
    "\n",
    "# FINAL NAME\n",
    "cluster_stats['cluster_name'] = (\n",
    "    cluster_stats['geo'] + \" ‚Äî \" +\n",
    "    cluster_stats['delay_text'] + \", \" +\n",
    "    cluster_stats['risk_text'] + \", \" +\n",
    "    cluster_stats['cong_text']\n",
    ")\n",
    "\n",
    "# Map into df\n",
    "name_map = cluster_stats.set_index('route_cluster')['cluster_name'].to_dict()\n",
    "df['cluster_desc'] = df['route_cluster'].map(name_map)\n",
    "\n",
    "cluster_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/11. Categorical columns ---\n",
    "# Identify which columns are categorical\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "print(\"Categorical columns:\", list(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/12. Categorical columns ---\n",
    "# Categorical columns ‚Üí replace NaN with most frequent value (mode)\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "print(\"\\n Missing values handled with NumPy (median/mode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/13. Save Cleaned Dataset ---\n",
    "df.to_csv('cleaned_logistics_data.csv', index=False)\n",
    "print(\"\\n Cleaned dataset saved as 'cleaned_logistics_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1/14. Final Dataset Summary ---\n",
    "print(\"\\nFinal dataset shape:\", df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "display(df.head())\n",
    "# print(df.shipping_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "# ====================================================\n",
    "\n",
    "# --- 2/1. Import Required Libraries ---\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Assume df is already loaded and cleaned from Phase 1\n",
    "print(\" DataFrame available with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40512d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/2. Route clusters visualization---\n",
    "# Ensure necessary columns exist\n",
    "required_cols = [\n",
    "    'vehicle_gps_latitude',\n",
    "    'vehicle_gps_longitude',\n",
    "    'route_cluster',\n",
    "    'delivery_time_deviation'\n",
    "]\n",
    "\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='vehicle_gps_longitude',\n",
    "    y='vehicle_gps_latitude',\n",
    "    hue='cluster_desc',      # ‚úî –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤ –ª–µ–≥–µ–Ω–¥–µ\n",
    "    size='delivery_time_deviation',\n",
    "    palette='tab10',\n",
    "    sizes=(40, 300),\n",
    "    alpha=0.8,\n",
    "    legend=\"brief\"\n",
    ")\n",
    "\n",
    "plt.title(\"Route Clusters Based on GPS Coordinates\\n(Bubble Size = Delivery Time Deviation)\", fontsize=16)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/3. Prepare stat data ---\n",
    "# Define the feature columns (independent variables)\n",
    "features = [\n",
    "    \"fuel_consumption_rate\", \"traffic_congestion_level\", \"weather_condition_severity\",\n",
    "    \"warehouse_inventory_level\", \"loading_unloading_time\", \"handling_equipment_availability\",\n",
    "    'route_cluster', \"port_congestion_level\", \"shipping_costs\", \"supplier_reliability_score\", \"\"\n",
    "    \"lead_time_days\", \"route_risk_level\", \"driver_behavior_score\", \"fatigue_monitoring_score\"\n",
    "]\n",
    "\n",
    "# Define the target column (dependent variable)\n",
    "# ===============================================\n",
    "target = \"delivery_time_deviation\"\n",
    "# ===============================================\n",
    "\n",
    "# Keep only the selected columns and remove rows with missing data\n",
    "stat_df = df[features + [target]].dropna()\n",
    "\n",
    "# Check the dataset after cleaning\n",
    "print(f\"Data after filtering: {stat_df.shape[0]:,} rows and {stat_df.shape[1]} columns\")\n",
    "\n",
    "# Display summary statistics of the cleaned dataset\n",
    "stat_df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee201516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/4. Encode categorical values ---\n",
    "\n",
    "# Exclude 'vehicle_city' from categorical encoding\n",
    "cat_cols_to_encode = [col for col in features if col != 'vehicle_city']\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "for col in cat_cols_to_encode:\n",
    "    df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Check result\n",
    "print(stat_df[cat_cols_to_encode].head())\n",
    "print(\"\\nCategorical encoding complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2692f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/5. Data Correlation ---\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assume stat_df has the same feature and target columns\n",
    "# -----------------------------\n",
    "# 1Ô∏è Spearman correlation (monotonic relationships)\n",
    "spearman_corr = stat_df.corr(method='spearman')[target].drop(target).sort_values(ascending=False)\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=spearman_corr.values, y=spearman_corr.index, hue=spearman_corr.index, dodge=False, palette=\"viridis\", legend=False)\n",
    "plt.title(\"Spearman Correlation with Delivery Time Deviation (stat_df)\")\n",
    "plt.xlabel(\"Spearman Correlation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è Mutual Information (non-linear dependencies)\n",
    "X_stat = stat_df[features]\n",
    "y_stat = stat_df[target]\n",
    "mi = mutual_info_regression(X_stat, y_stat, random_state=42)\n",
    "mi_series = pd.Series(mi, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    x=mi_series.values, \n",
    "    y=mi_series.index, \n",
    "    dodge=False, palette=\"viridis\", legend=False\n",
    ")\n",
    "plt.title(\"Mutual Information with Delivery Time Deviation (stat_df)\")\n",
    "plt.xlabel(\"Mutual Information\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è Feature importance using Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_stat, y_stat)\n",
    "rf_importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=rf_importances.values, y=rf_importances.index, hue=spearman_corr.index, dodge=False, palette=\"magma\", legend=False)\n",
    "# sns.barplot(x=rf_importances.values, y=rf_importances.index, palette=\"magma\")\n",
    "plt.title(\"Random Forest Feature Importance (stat_df)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3Ô∏è DISTRIBUTION ANALYSIS AND ANOMALY DETECTION\n",
    "# ====================================================\n",
    "# --- 3/1. Plot distribution of key numeric variables---\n",
    "\n",
    "key_vars = [\n",
    "    'delivery_time_deviation',\n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "key_vars = [v for v in key_vars if v in df.columns]\n",
    "\n",
    "for col in key_vars:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff98113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/1-1 Summary statistics by cluster_desc\n",
    "summary_by_cluster = df.groupby('cluster_desc').agg(\n",
    "    avg_delay=('delivery_time_deviation', 'mean'),\n",
    "    avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "    avg_fuel=('fuel_consumption_rate', 'mean'),\n",
    "    avg_shipping=('shipping_costs', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot: Average delivery delay by cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='avg_delay',\n",
    "    y='cluster_desc',\n",
    "    data=summary_by_cluster.sort_values('avg_delay', ascending=False),\n",
    "    palette='Reds_r'\n",
    ")\n",
    "plt.title(\"Average Delivery Delay by Cluster\")\n",
    "plt.xlabel(\"Average Delivery Time Deviation (hours)\")\n",
    "plt.ylabel(\"Cluster Description\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Multi-metric plot using seaborn's barplot in subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,10))\n",
    "metrics = ['avg_delay', 'avg_congestion', 'avg_fuel', 'avg_shipping']\n",
    "titles = ['Avg Delivery Delay', 'Avg Traffic Congestion', 'Avg Fuel Consumption', 'Avg Shipping Cost']\n",
    "colors = ['Reds', 'Blues', 'Greens', 'Purples']\n",
    "\n",
    "for ax, metric, title, color in zip(axes.flatten(), metrics, titles, colors):\n",
    "    sns.barplot(\n",
    "        x=metric,\n",
    "        y='cluster_desc',\n",
    "        data=summary_by_cluster.sort_values(metric, ascending=False),\n",
    "        palette=color + \"_r\",\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/1-1 Summary statistics by cluster_desc\n",
    "summary_by_cluster = df.groupby('cluster_desc').agg(\n",
    "    avg_delay=('delivery_time_deviation', 'mean'),\n",
    "    avg_congestion=('traffic_congestion_level', 'mean'),\n",
    "    avg_fuel=('fuel_consumption_rate', 'mean'),\n",
    "    avg_shipping=('shipping_costs', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot: Average delivery delay by cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='avg_delay',\n",
    "    y='cluster_desc',\n",
    "    hue='cluster_desc',        # set hue same as y\n",
    "    data=summary_by_cluster.sort_values('avg_delay', ascending=False),\n",
    "    palette='Reds_r',\n",
    "    dodge=False,               # avoid splitting bars\n",
    "    legend=False               # hide the extra legend\n",
    ")\n",
    "plt.title(\"Average Delivery Delay by Cluster\")\n",
    "plt.xlabel(\"Average Delivery Time Deviation (hours)\")\n",
    "plt.ylabel(\"Cluster Description\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/2. Detect anomalies using IQR method for each key numeric column---\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    q1, q3 = np.percentile(series.dropna(), [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    return ((series < lower) | (series > upper)).sum()\n",
    "\n",
    "outlier_summary = {col: detect_outliers_iqr(df[col]) for col in key_vars}\n",
    "print(\"\\n Outlier count per key variable:\")\n",
    "print(pd.Series(outlier_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 4Ô∏è SEASONAL & REGIONAL DELAY TRENDS\n",
    "# ====================================================\n",
    "# --- 4/1. fuel consumption rate by month---\n",
    "\n",
    "#correlation between fuel consuntion rate and temperature inside\n",
    "\n",
    "\n",
    "# Convert timestamp to datetime if needed\n",
    "if not np.issubdtype(df['timestamp'].dtype, np.datetime64):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Extract time-based features\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# --- 4.1 Seasonal trends (monthly average delay)\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    monthly_delay = df.groupby('month')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=monthly_delay.index, y=monthly_delay.values, marker='o')\n",
    "    plt.title(\" Average Delivery Time Deviation by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 Regional trends (by city or location if available)\n",
    "\n",
    "if 'vehicle_city' in df.columns:\n",
    "    regional_delay = df.groupby('vehicle_city')['delivery_time_deviation'].mean().sort_values(ascending=False).head(10)\n",
    "    # regional_delay = df.groupby('vehicle_city')['delivery_time_deviation'].mean().sort_values(ascending=True).head(10)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(\n",
    "    x=regional_delay.values,\n",
    "    y=regional_delay.index,\n",
    "    hue=regional_delay.values,  # use values to color bars\n",
    "    palette='coolwarm',\n",
    "    legend=False               # hide legend\n",
    "    )\n",
    "    # plt.xticks(rotation=45, ha='right')  # 45 degrees, align right\n",
    "    plt.title(\" Top 10 Cities by Average Delivery Deviation\")\n",
    "    plt.xlabel(\"Average Delay\")\n",
    "    plt.ylabel(\"City\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n No 'vehicle_city' column found for regional analysis. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6429ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4.2 Regional trends (table)\n",
    "# Group by city and calculate average delivery delay\n",
    "regional_delay = (\n",
    "    df.groupby('vehicle_city')['delivery_time_deviation']\n",
    "      .mean()\n",
    "      .sort_values(ascending=False)\n",
    "      .head(10)\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "regional_delay_df = regional_delay.reset_index()\n",
    "regional_delay_df.columns = ['City', 'Avg_Delivery_Time_Deviation']\n",
    "\n",
    "# Round the values for better readability\n",
    "regional_delay_df['Avg_Delivery_Time_Deviation'] = regional_delay_df['Avg_Delivery_Time_Deviation'].round(2)\n",
    "\n",
    "# Display table\n",
    "regional_delay_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Weekday delay trends\n",
    "\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "# Define correct order\n",
    "    weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "    # Convert to categorical with order\n",
    "    df['weekday'] = pd.Categorical(df['weekday'], categories=weekday_order, ordered=True)\n",
    "\n",
    "    # Group and sort\n",
    "    weekday_delay = df.groupby('weekday')['delivery_time_deviation'].mean().sort_index()\n",
    "    # print(weekday_delay)\n",
    "\n",
    "    # weekday_delay = df.groupby('weekday')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    sns.barplot(\n",
    "    x=weekday_delay.index,\n",
    "    y=weekday_delay.values,\n",
    "    hue=weekday_delay.values,  # use values to color bars\n",
    "    palette='crest',\n",
    "    legend=False               # hide legend\n",
    "    )\n",
    "\n",
    "    plt.title(\" Average Delivery Deviation by Weekday\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Phase 2: EDA completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Scale Numerical Features ---\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "print(\"\\n Numerical features scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4 disruption_likelihood_score ---\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Preprocessing\n",
    "features = [\n",
    "    'vehicle_gps_latitude', 'vehicle_gps_longitude', 'fuel_consumption_rate',\n",
    "    'eta_variation_hours', 'traffic_congestion_level', 'warehouse_inventory_level',\n",
    "    'loading_unloading_time', 'handling_equipment_availability',\n",
    "    'weather_condition_severity', 'port_congestion_level', 'shipping_costs',\n",
    "    'supplier_reliability_score', 'lead_time_days', 'historical_demand',\n",
    "    'iot_temperature', 'cargo_condition_status', 'route_risk_level',\n",
    "    'customs_clearance_time', 'driver_behavior_score', 'fatigue_monitoring_score'\n",
    "]\n",
    "target = 'disruption_likelihood_score'  # Adjust this if needed based on your problem\n",
    "\n",
    "\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 5: Machine Learning Modeling\n",
    "# ====================================================\n",
    "# 5/1 FEATURE SELECTION & ENCODING\n",
    "\n",
    "# Drop non-numeric / irrelevant columns\n",
    "exclude_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname']\n",
    "df = df.drop(columns=[c for c in exclude_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Replace remaining NaNs with median\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 DATA CLEANUP BEFORE MODELING\n",
    "\n",
    "\n",
    "# Work on a copy to preserve df\n",
    "data = df.copy()\n",
    "\n",
    "# Drop identifier or irrelevant columns if they exist\n",
    "drop_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email',\n",
    "    'Customer Fname', 'Customer Lname'\n",
    "]\n",
    "data = data.drop(columns=[c for c in drop_cols if c in data.columns], errors='ignore')\n",
    "\n",
    "# --- 1. Convert booleans to int\n",
    "bool_cols = data.select_dtypes(include=['bool']).columns\n",
    "if len(bool_cols):\n",
    "    data[bool_cols] = data[bool_cols].astype(int)\n",
    "\n",
    "# --- 2. Encode categoricals (object / category)\n",
    "cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "if len(cat_cols):\n",
    "    print(\"Encoding categorical columns:\", list(cat_cols))\n",
    "    for col in cat_cols:\n",
    "        data[col] = data[col].astype(str).astype('category').cat.codes\n",
    "\n",
    "# --- 3. Replace any infinite or missing values\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.fillna(data.median(numeric_only=True))\n",
    "\n",
    "# --- 4. Enforce numeric dtype globally\n",
    "data = data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# --- 5. Verify everything is numeric\n",
    "non_numeric = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\" Still non-numeric columns:\", non_numeric)\n",
    "else:\n",
    "    print(\" All columns successfully converted to numeric.\")\n",
    "\n",
    "#  Ready for modeling\n",
    "df = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5/2: Model Training with Progress Bar ---\n",
    "\n",
    "#!pip install tqdm --quiet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from tqdm.notebook import tqdm  # progress bar for Jupyter\n",
    "\n",
    "# --- Example target columns ---\n",
    "target_reg = \"delivery_time_deviation\"\n",
    "target_cls = \"risk_classification\"\n",
    "\n",
    "# --- Ensure numeric features only ---\n",
    "X = df.select_dtypes(include=[np.number])\n",
    "y_reg = df[target_reg]\n",
    "y_cls = df[target_cls].astype(str)  # ensure string for classification\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Models ---\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Results dict ---\n",
    "results_reg = {}\n",
    "\n",
    "# --- Model training with progress bar ---\n",
    "print(\" Training regression models...\")\n",
    "for name in tqdm(models_reg.keys(), desc=\"Training Progress\", leave=False):\n",
    "    model = models_reg[name]\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "    print(f\"\\n {name} done.\")\n",
    "    print(f\"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "# --- Display summary ---\n",
    "results_reg_df = pd.DataFrame(results_reg).T\n",
    "display(results_reg_df.style.background_gradient(cmap=\"Blues\").format(\"{:.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5/2: Global storage for trained models---\n",
    "trained_models = {\n",
    "    \"regression\": {},\n",
    "    \"classification\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5/3: Machine Learning Modeling (Optimized)---\n",
    "\\\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "\n",
    "# --- 1Ô∏è Preprocessing: ensure numeric ---\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Drop identifier columns\n",
    "drop_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname']\n",
    "df_ml = df_ml.drop(columns=[c for c in drop_cols if c in df_ml.columns], errors='ignore')\n",
    "\n",
    "# Encode booleans\n",
    "bool_cols = df_ml.select_dtypes(include='bool').columns\n",
    "df_ml[bool_cols] = df_ml[bool_cols].astype(int)\n",
    "\n",
    "# Encode categorical columns\n",
    "cat_cols = df_ml.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df_ml[col] = LabelEncoder().fit_transform(df_ml[col].astype(str))\n",
    "\n",
    "# Fill NaNs\n",
    "df_ml = df_ml.fillna(df_ml.median(numeric_only=True))\n",
    "\n",
    "print(\" Preprocessing complete. Data ready for modeling.\")\n",
    "\n",
    "# --- 2Ô∏è Regression: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\n Regression: delivery_time_deviation\")\n",
    "\n",
    "    X_reg = df_ml.drop(columns=['delivery_time_deviation'])\n",
    "    y_reg = df_ml['delivery_time_deviation']\n",
    "\n",
    "    # --- Optional: sample for speed ---\n",
    "    sample_size = 5000\n",
    "    if len(X_reg) > sample_size:\n",
    "        X_reg = X_reg.sample(sample_size, random_state=42)\n",
    "        y_reg = y_reg.loc[X_reg.index]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_reg = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest Regressor\": RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "    }\n",
    "\n",
    "    results_reg = {}\n",
    "    for name in tqdm(models_reg.keys(), desc=\"Regression Models\", leave=False):\n",
    "        model = models_reg[name]\n",
    "        start = pd.Timestamp.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "        print(f\"\\n {name} done in {elapsed:.2f} sec | MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "        # Feature importance for Random Forest\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feat_imp = pd.Series(model.feature_importances_, index=X_reg.columns)\n",
    "            feat_imp.nlargest(10).plot(kind='barh', figsize=(8,4), title=f\"Top 10 Feature Importances ({name})\")\n",
    "            plt.show()\n",
    "\n",
    "# --- 3Ô∏è Classification: risk_classification & delay_probability ---\n",
    "def run_classification(target_col):\n",
    "    print(f\"\\n Classification: {target_col}\")\n",
    "    if target_col not in df_ml.columns:\n",
    "        print(f\" Column '{target_col}' not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    X = df_ml.drop(columns=[target_col])\n",
    "    y = df_ml[target_col]\n",
    "\n",
    "    # Optional sampling\n",
    "    sample_size = 5000\n",
    "    if len(X) > sample_size:\n",
    "        X = X.sample(sample_size, random_state=42)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    # Encode target\n",
    "    y = LabelEncoder().fit_transform(y.astype(str))\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_cls = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "    }\n",
    "\n",
    "    for name in tqdm(models_cls.keys(), desc=f\"{target_col} Models\", leave=False):\n",
    "        model = models_cls[name]\n",
    "        start = pd.Timestamp.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        # --- Save trained model globally ---\n",
    "        trained_models['classification'][f\"{target}_{name}\"] = model\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(f\"\\n {name} done in {elapsed:.2f} sec | Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Run classification tasks\n",
    "for target in ['risk_classification', 'delay_probability']:\n",
    "    run_classification(target)\n",
    "\n",
    "print(\"\\n Phase 3: Machine Learning Modeling completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2942901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 6: Insights & Recommendations\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "print(\" Phase 4: Interpreting model outputs and generating recommendations\")\n",
    "\n",
    "# --- 1Ô∏è Regression Insights: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\nüîπ Regression Insights: Top drivers of delivery time deviation\")\n",
    "\n",
    "    # Use the last trained Random Forest model from Phase 3\n",
    "    rf_model = models_reg.get(\"Random Forest Regressor\", None)\n",
    "\n",
    "    if rf_model is not None:\n",
    "        # Feature importance\n",
    "        feat_imp = pd.Series(rf_model.feature_importances_, index=X_reg.columns).sort_values(ascending=False)\n",
    "        top_features = feat_imp.head(10)\n",
    "        print(\"\\nTop 10 features driving delivery time deviation:\")\n",
    "        display(top_features)\n",
    "\n",
    "        # Visualize\n",
    "        # sns.barplot(\n",
    "        #     x=top_features.values,\n",
    "        #     y=top_features.index,\n",
    "        #     hue=top_features.index,     # color by the same variable\n",
    "        #     palette=\"viridis\",\n",
    "        #     dodge=False,\n",
    "        #     legend=False\n",
    "        # )\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\n",
    "        plt.title(\"Top 10 Feature Importances ‚Äî Delivery Time Deviation\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features.index:\n",
    "            print(f\"- Monitor and optimize {feat} to reduce delivery time deviations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a194042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6/2. Classification Insights ---\n",
    "for target in ['risk_classification', 'delay_probability']:\n",
    "    print(f\"\\nüîπ Classification Insights: {target}\")\n",
    "\n",
    "    # Access the trained Random Forest Classifier from the global dictionary\n",
    "    rf_model_cls = trained_models['classification'].get(f\"{target}_Random Forest Classifier\", None)\n",
    "\n",
    "    if rf_model_cls is not None:\n",
    "        # Ensure we use the same features as in training\n",
    "        X_train_cls = df_ml.select_dtypes(include=[np.number]).drop(columns=[target], errors='ignore')\n",
    "\n",
    "        # Feature importance\n",
    "        feat_imp_cls = pd.Series(rf_model_cls.feature_importances_, index=X_train_cls.columns).sort_values(ascending=False)\n",
    "        top_features_cls = feat_imp_cls.head(10)\n",
    "\n",
    "        print(\"\\nTop 10 features driving classification outcome:\")\n",
    "        display(top_features_cls)\n",
    "\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(x=top_features_cls.values, y=top_features_cls.index, palette=\"magma\")\n",
    "        plt.title(f\"Top 10 Feature Importances ‚Äî {target}\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features_cls.index:\n",
    "            print(f\"- Investigate and improve {feat} to reduce {target.replace('_',' ')} risks.\")\n",
    "    else:\n",
    "        print(f\" No trained model found for {target}, skipping.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autowini_venv)",
   "language": "python",
   "name": "autowini_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
