{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 1: Data Preparation\n",
    "# Project: Predictive Analytics for Supply Chain Optimization\n",
    "# ====================================================\n",
    "\n",
    "# --- 1. Import Required Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6910ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "  df = pd.read_csv(r\"C:\\Users\\BOB\\Documents\\Data Analysis\\.data\\dynamic_supply_chain_logistics_dataset.csv\", encoding='latin-1')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Show list of columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Basic Data Overview ---\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nMissing values summary:\\n\", df.isna().sum())\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Handle Missing Values with NumPy ---\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "# Numeric columns ‚Üí replace NaN with column median using NumPy\n",
    "for col in num_cols:\n",
    "    median_val = np.nanmedian(df[col])\n",
    "    df[col] = df[col].fillna(median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns ‚Üí replace NaN with most frequent value (mode)\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "print(\"\\n Missing values handled with NumPy (median/mode).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Feature Engineering ---\n",
    "\n",
    "## 5.1 Create derived features\n",
    "# Example: combine GPS into one column\n",
    "df['vehicle_location'] = df['vehicle_gps_latitude'].astype(str) + ',' + df['vehicle_gps_longitude'].astype(str)\n",
    "\n",
    "# Example: compute delay difference in minutes\n",
    "df['eta_variation_minutes'] = df['eta_variation_hours'] * 60\n",
    "\n",
    "# Example: categorize traffic level\n",
    "df['traffic_category'] = pd.cut(\n",
    "    df['traffic_congestion_level'],\n",
    "    bins=[0, 3, 6, 10],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Example: flag if weather severity exceeds threshold\n",
    "df['severe_weather_flag'] = np.where(df['weather_condition_severity'] > 7, 1, 0)\n",
    "\n",
    "print(\"\\n Feature engineering complete. New columns added:\", \n",
    "      [c for c in df.columns if 'vehicle_location' in c or 'eta_variation_minutes' in c or 'traffic_category' in c or 'severe_weather_flag' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21408231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Encode Categorical Features ---\n",
    "label_enc = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "\n",
    "print(\"\\n Categorical encoding complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Remove Duplicates ---\n",
    "initial_shape = df.shape\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"\\n Removed {initial_shape[0] - df.shape[0]} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Scale Numerical Features ---\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "print(\"\\n Numerical features scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Final Dataset Summary ---\n",
    "print(\"\\nFinal dataset shape:\", df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. Save Cleaned Dataset ---\n",
    "df.to_csv('cleaned_logistics_data.csv', index=False)\n",
    "print(\"\\n Cleaned dataset saved as 'cleaned_logistics_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "# ====================================================\n",
    "\n",
    "# --- 1. Import Required Libraries ---\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Assume df is already loaded and cleaned from Phase 1\n",
    "print(\" DataFrame available with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 2 CORRELATION ANALYSIS\n",
    "# ====================================================\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Display top correlated features with delivery deviation\n",
    "target = 'delivery_time_deviation'\n",
    "if target in corr_matrix.columns:\n",
    "    print(\"\\n Top 10 correlations with delivery_time_deviation:\")\n",
    "    print(corr_matrix[target].abs().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Heatmap visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3Ô∏è DISTRIBUTION ANALYSIS AND ANOMALY DETECTION\n",
    "# ====================================================\n",
    "\n",
    "# Plot distribution of key numeric variables\n",
    "key_vars = [\n",
    "    'delivery_time_deviation',\n",
    "    'fuel_consumption_rate',\n",
    "    'traffic_congestion_level',\n",
    "    'shipping_costs',\n",
    "    'lead_time_days'\n",
    "]\n",
    "key_vars = [v for v in key_vars if v in df.columns]\n",
    "\n",
    "for col in key_vars:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using IQR method for each key numeric column\n",
    "def detect_outliers_iqr(series):\n",
    "    q1, q3 = np.percentile(series.dropna(), [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    return ((series < lower) | (series > upper)).sum()\n",
    "\n",
    "outlier_summary = {col: detect_outliers_iqr(df[col]) for col in key_vars}\n",
    "print(\"\\nüîé Outlier count per key variable:\")\n",
    "print(pd.Series(outlier_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 4Ô∏è SEASONAL & REGIONAL DELAY TRENDS\n",
    "# ====================================================\n",
    "\n",
    "# Convert timestamp to datetime if needed\n",
    "if not np.issubdtype(df['timestamp'].dtype, np.datetime64):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Extract time-based features\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# --- 4.1 Seasonal trends (monthly average delay)\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    monthly_delay = df.groupby('month')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=monthly_delay.index, y=monthly_delay.values, marker='o')\n",
    "    plt.title(\" Average Delivery Time Deviation by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 Regional trends (by city or location if available)\n",
    "if 'Customer City' in df.columns:\n",
    "    regional_delay = df.groupby('Customer City')['delivery_time_deviation'].mean().sort_values(ascending=False).head(10)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=regional_delay.values, y=regional_delay.index, palette='coolwarm')\n",
    "    plt.title(\" Top 10 Cities by Average Delivery Deviation\")\n",
    "    plt.xlabel(\"Average Delay\")\n",
    "    plt.ylabel(\"City\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n No 'Customer City' column found for regional analysis. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 Weekday delay trends\n",
    "if 'delivery_time_deviation' in df.columns:\n",
    "    weekday_delay = df.groupby('weekday')['delivery_time_deviation'].mean()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=weekday_delay.index, y=weekday_delay.values, palette='crest')\n",
    "    plt.title(\" Average Delivery Deviation by Weekday\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.ylabel(\"Avg Delivery Deviation\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Phase 2: EDA completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 3: Machine Learning Modeling\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "\n",
    "# Assume df is already loaded and cleaned from previous phases\n",
    "print(\" DataFrame ready with shape:\", df.shape)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2Ô∏è FEATURE SELECTION & ENCODING\n",
    "# ====================================================\n",
    "\n",
    "# Drop non-numeric / irrelevant columns\n",
    "exclude_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname']\n",
    "df = df.drop(columns=[c for c in exclude_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Replace remaining NaNs with median\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop non-relevant identifier columns (keep numeric & categorical features only)\n",
    "exclude_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email',\n",
    "    'Customer Fname', 'Customer Lname'\n",
    "]\n",
    "df = df.drop(columns=[c for c in exclude_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(\"Categorical columns:\", list(cat_cols))\n",
    "print(\"Numeric columns:\", list(num_cols))\n",
    "\n",
    "# Encode categorical columns safely using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Fill NaN with median for numeric, mode for categorical\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "print(\"All categorical features encoded successfully. Ready for modeling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# üîß DATA CLEANUP BEFORE MODELING ‚Äî FINAL SAFE VERSION\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# Work on a copy to preserve df\n",
    "data = df.copy()\n",
    "\n",
    "# Drop identifier or irrelevant columns if they exist\n",
    "drop_cols = [\n",
    "    'timestamp', 'Customer City', 'Customer Email',\n",
    "    'Customer Fname', 'Customer Lname'\n",
    "]\n",
    "data = data.drop(columns=[c for c in drop_cols if c in data.columns], errors='ignore')\n",
    "\n",
    "# --- 1. Convert booleans to int\n",
    "bool_cols = data.select_dtypes(include=['bool']).columns\n",
    "if len(bool_cols):\n",
    "    data[bool_cols] = data[bool_cols].astype(int)\n",
    "\n",
    "# --- 2. Encode categoricals (object / category)\n",
    "cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "if len(cat_cols):\n",
    "    print(\"Encoding categorical columns:\", list(cat_cols))\n",
    "    for col in cat_cols:\n",
    "        data[col] = data[col].astype(str).astype('category').cat.codes\n",
    "\n",
    "# --- 3. Replace any infinite or missing values\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.fillna(data.median(numeric_only=True))\n",
    "\n",
    "# --- 4. Enforce numeric dtype globally\n",
    "data = data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# --- 5. Verify everything is numeric\n",
    "non_numeric = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\" Still non-numeric columns:\", non_numeric)\n",
    "else:\n",
    "    print(\" All columns successfully converted to numeric.\")\n",
    "\n",
    "#  Ready for modeling\n",
    "df = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 3: Model Training with Progress Bar ---\n",
    "\n",
    "#!pip install tqdm --quiet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from tqdm.notebook import tqdm  # progress bar for Jupyter\n",
    "\n",
    "# --- Example target columns ---\n",
    "target_reg = \"delivery_time_deviation\"\n",
    "target_cls = \"risk_classification\"\n",
    "\n",
    "# --- Ensure numeric features only ---\n",
    "X = df.select_dtypes(include=[np.number])\n",
    "y_reg = df[target_reg]\n",
    "y_cls = df[target_cls].astype(str)  # ensure string for classification\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Models ---\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=50, max_depth=10, n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Results dict ---\n",
    "results_reg = {}\n",
    "\n",
    "# --- Model training with progress bar ---\n",
    "print(\" Training regression models...\")\n",
    "for name in tqdm(models_reg.keys(), desc=\"Training Progress\", leave=False):\n",
    "    model = models_reg[name]\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "    print(f\"\\n {name} done.\")\n",
    "    print(f\"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "# --- Display summary ---\n",
    "results_reg_df = pd.DataFrame(results_reg).T\n",
    "display(results_reg_df.style.background_gradient(cmap=\"Blues\").format(\"{:.3f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global storage for trained models\n",
    "trained_models = {\n",
    "    \"regression\": {},\n",
    "    \"classification\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 3: Machine Learning Modeling (Optimized)\n",
    "# ====================================================\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "\n",
    "# --- 1Ô∏è Preprocessing: ensure numeric ---\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Drop identifier columns\n",
    "drop_cols = ['timestamp', 'Customer City', 'Customer Email', 'Customer Fname', 'Customer Lname']\n",
    "df_ml = df_ml.drop(columns=[c for c in drop_cols if c in df_ml.columns], errors='ignore')\n",
    "\n",
    "# Encode booleans\n",
    "bool_cols = df_ml.select_dtypes(include='bool').columns\n",
    "df_ml[bool_cols] = df_ml[bool_cols].astype(int)\n",
    "\n",
    "# Encode categorical columns\n",
    "cat_cols = df_ml.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df_ml[col] = LabelEncoder().fit_transform(df_ml[col].astype(str))\n",
    "\n",
    "# Fill NaNs\n",
    "df_ml = df_ml.fillna(df_ml.median(numeric_only=True))\n",
    "\n",
    "print(\" Preprocessing complete. Data ready for modeling.\")\n",
    "\n",
    "# --- 2Ô∏è Regression: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\n Regression: delivery_time_deviation\")\n",
    "\n",
    "    X_reg = df_ml.drop(columns=['delivery_time_deviation'])\n",
    "    y_reg = df_ml['delivery_time_deviation']\n",
    "\n",
    "    # --- Optional: sample for speed ---\n",
    "    sample_size = 5000\n",
    "    if len(X_reg) > sample_size:\n",
    "        X_reg = X_reg.sample(sample_size, random_state=42)\n",
    "        y_reg = y_reg.loc[X_reg.index]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_reg = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest Regressor\": RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "    }\n",
    "\n",
    "    results_reg = {}\n",
    "    for name in tqdm(models_reg.keys(), desc=\"Regression Models\", leave=False):\n",
    "        model = models_reg[name]\n",
    "        start = pd.Timestamp.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        results_reg[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R¬≤\": r2}\n",
    "        print(f\"\\n {name} done in {elapsed:.2f} sec | MAE: {mae:.3f} | RMSE: {rmse:.3f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "        # Feature importance for Random Forest\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feat_imp = pd.Series(model.feature_importances_, index=X_reg.columns)\n",
    "            feat_imp.nlargest(10).plot(kind='barh', figsize=(8,4), title=f\"Top 10 Feature Importances ({name})\")\n",
    "            plt.show()\n",
    "\n",
    "# --- 3Ô∏è Classification: risk_classification & delay_probability ---\n",
    "def run_classification(target_col):\n",
    "    print(f\"\\n Classification: {target_col}\")\n",
    "    if target_col not in df_ml.columns:\n",
    "        print(f\" Column '{target_col}' not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    X = df_ml.drop(columns=[target_col])\n",
    "    y = df_ml[target_col]\n",
    "\n",
    "    # Optional sampling\n",
    "    sample_size = 5000\n",
    "    if len(X) > sample_size:\n",
    "        X = X.sample(sample_size, random_state=42)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    # Encode target\n",
    "    y = LabelEncoder().fit_transform(y.astype(str))\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Models\n",
    "    models_cls = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "    }\n",
    "\n",
    "    for name in tqdm(models_cls.keys(), desc=f\"{target_col} Models\", leave=False):\n",
    "        model = models_cls[name]\n",
    "        start = pd.Timestamp.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        # --- Save trained model globally ---\n",
    "        trained_models['classification'][f\"{target}_{name}\"] = model\n",
    "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(f\"\\n {name} done in {elapsed:.2f} sec | Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Run classification tasks\n",
    "for target in ['risk_classification', 'delay_probability']:\n",
    "    run_classification(target)\n",
    "\n",
    "print(\"\\n Phase 3: Machine Learning Modeling completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2942901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Phase 4: Insights & Recommendations\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "print(\" Phase 4: Interpreting model outputs and generating recommendations\")\n",
    "\n",
    "# --- 1Ô∏è Regression Insights: delivery_time_deviation ---\n",
    "if 'delivery_time_deviation' in df_ml.columns:\n",
    "    print(\"\\nüîπ Regression Insights: Top drivers of delivery time deviation\")\n",
    "\n",
    "    # Use the last trained Random Forest model from Phase 3\n",
    "    rf_model = models_reg.get(\"Random Forest Regressor\", None)\n",
    "\n",
    "    if rf_model is not None:\n",
    "        # Feature importance\n",
    "        feat_imp = pd.Series(rf_model.feature_importances_, index=X_reg.columns).sort_values(ascending=False)\n",
    "        top_features = feat_imp.head(10)\n",
    "        print(\"\\nTop 10 features driving delivery time deviation:\")\n",
    "        display(top_features)\n",
    "\n",
    "        # Visualize\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\n",
    "        plt.title(\"Top 10 Feature Importances ‚Äî Delivery Time Deviation\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features.index:\n",
    "            print(f\"- Monitor and optimize {feat} to reduce delivery time deviations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a194042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Insights ---\n",
    "for target in ['risk_classification', 'delay_probability']:\n",
    "    print(f\"\\nüîπ Classification Insights: {target}\")\n",
    "\n",
    "    # Access the trained Random Forest Classifier from the global dictionary\n",
    "    rf_model_cls = trained_models['classification'].get(f\"{target}_Random Forest Classifier\", None)\n",
    "\n",
    "    if rf_model_cls is not None:\n",
    "        # Ensure we use the same features as in training\n",
    "        X_train_cls = df_ml.select_dtypes(include=[np.number]).drop(columns=[target], errors='ignore')\n",
    "\n",
    "        # Feature importance\n",
    "        feat_imp_cls = pd.Series(rf_model_cls.feature_importances_, index=X_train_cls.columns).sort_values(ascending=False)\n",
    "        top_features_cls = feat_imp_cls.head(10)\n",
    "\n",
    "        print(\"\\nTop 10 features driving classification outcome:\")\n",
    "        display(top_features_cls)\n",
    "\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(x=top_features_cls.values, y=top_features_cls.index, palette=\"magma\")\n",
    "        plt.title(f\"Top 10 Feature Importances ‚Äî {target}\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\n Recommendations based on top features:\")\n",
    "        for feat in top_features_cls.index:\n",
    "            print(f\"- Investigate and improve {feat} to reduce {target.replace('_',' ')} risks.\")\n",
    "    else:\n",
    "        print(f\" No trained model found for {target}, skipping.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autowini_venv)",
   "language": "python",
   "name": "autowini_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
